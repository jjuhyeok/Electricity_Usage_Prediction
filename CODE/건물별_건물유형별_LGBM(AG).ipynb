{"cells":[{"cell_type":"markdown","metadata":{"id":"S7Fxwl6kfmZR"},"source":["# xgb alpha"]},{"cell_type":"markdown","metadata":{"id":"3QJjv70IfmZR"},"source":["## preprocess"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"i0nVFTLAfmZS"},"outputs":[],"source":["import random\n","import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","from autogluon.core.metrics import make_scorer\n","from autogluon.tabular import TabularDataset, TabularPredictor\n","\n","from sklearn.model_selection import StratifiedKFold\n","\n","from sklearn.ensemble import RandomForestRegressor\n","from lightgbm import LGBMRegressor\n","from xgboost import XGBRegressor\n","\n","os.makedirs('subs', exist_ok=True)\n","# import warnings\n","# warnings.filterwarnings(action='ignore')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"91dhKnj-fmZS"},"outputs":[],"source":["seed = 21011928\n","#724, 990313, 21011928\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","\n","seed_everything(seed) # Seed 고정"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1790,"status":"ok","timestamp":1693037082389,"user":{"displayName":"박여오","userId":"10243965647474797689"},"user_tz":-540},"id":"7177Soj3fmZT","outputId":"2390f339-4192-4e99-8893-afd6723eeab9"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>num_date_time</th>\n","      <th>건물번호</th>\n","      <th>일시</th>\n","      <th>기온(C)</th>\n","      <th>강수량(mm)</th>\n","      <th>풍속(m/s)</th>\n","      <th>습도(%)</th>\n","      <th>일조(hr)</th>\n","      <th>일사(MJ/m2)</th>\n","      <th>전력소비량(kWh)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1_20220601 00</td>\n","      <td>1</td>\n","      <td>20220601 00</td>\n","      <td>18.6</td>\n","      <td>NaN</td>\n","      <td>0.9</td>\n","      <td>42.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1085.28</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1_20220601 01</td>\n","      <td>1</td>\n","      <td>20220601 01</td>\n","      <td>18.0</td>\n","      <td>NaN</td>\n","      <td>1.1</td>\n","      <td>45.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1047.36</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1_20220601 02</td>\n","      <td>1</td>\n","      <td>20220601 02</td>\n","      <td>17.7</td>\n","      <td>NaN</td>\n","      <td>1.5</td>\n","      <td>45.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>974.88</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1_20220601 03</td>\n","      <td>1</td>\n","      <td>20220601 03</td>\n","      <td>16.7</td>\n","      <td>NaN</td>\n","      <td>1.4</td>\n","      <td>48.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>953.76</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1_20220601 04</td>\n","      <td>1</td>\n","      <td>20220601 04</td>\n","      <td>18.4</td>\n","      <td>NaN</td>\n","      <td>2.8</td>\n","      <td>43.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>986.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   num_date_time  건물번호           일시  기온(C)  강수량(mm)  풍속(m/s)  습도(%)  일조(hr)  \\\n","0  1_20220601 00     1  20220601 00   18.6      NaN      0.9   42.0     NaN   \n","1  1_20220601 01     1  20220601 01   18.0      NaN      1.1   45.0     NaN   \n","2  1_20220601 02     1  20220601 02   17.7      NaN      1.5   45.0     NaN   \n","3  1_20220601 03     1  20220601 03   16.7      NaN      1.4   48.0     NaN   \n","4  1_20220601 04     1  20220601 04   18.4      NaN      2.8   43.0     NaN   \n","\n","   일사(MJ/m2)  전력소비량(kWh)  \n","0        NaN     1085.28  \n","1        NaN     1047.36  \n","2        NaN      974.88  \n","3        NaN      953.76  \n","4        NaN      986.40  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","infos = pd.read_csv('building_info.csv')\n","infos = infos.replace('-', np.NaN)\n","infos[['태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)']] = infos[['태양광용량(kW)', 'ESS저장용량(kWh)', 'PCS용량(kW)']].astype(float)\n","train_df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1693037082390,"user":{"displayName":"박여오","userId":"10243965647474797689"},"user_tz":-540},"id":"VWP87FOVfmZT","outputId":"c3eb1d24-822f-4759-caee-730535cf9414"},"outputs":[{"data":{"text/plain":["0    64\n","1    36\n","Name: 태양광용량(kW), dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["infos['태양광용량(kW)'] = infos['태양광용량(kW)'].fillna(0)\n","infos['태양광용량(kW)'] = np.where(infos['태양광용량(kW)'] > 0, 1, 0)\n","infos['태양광용량(kW)'].value_counts()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1693037082390,"user":{"displayName":"박여오","userId":"10243965647474797689"},"user_tz":-540},"id":"vY9h0aj9fmZT","outputId":"4cc4c463-cf2c-4c7f-ccf3-733d73f4380e"},"outputs":[{"data":{"text/plain":["건물번호             0\n","건물유형             0\n","연면적(m2)          0\n","냉방면적(m2)         0\n","태양광용량(kW)        0\n","ESS저장용량(kWh)    95\n","PCS용량(kW)       95\n","dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["infos.isnull().sum()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"gOoTmfDcfmZU"},"outputs":[],"source":["train_df = pd.merge(train_df, infos[['건물번호', '건물유형', '태양광용량(kW)']], how = 'outer',on = '건물번호')\n","test_df = pd.merge(test_df, infos[['건물번호', '건물유형', '태양광용량(kW)']], how = 'outer',on = '건물번호')\n","# train_df = pd.merge(train_df, infos[['건물번호', '건물유형', '연면적(m2)', '냉방면적(m2)']], how = 'outer',on = '건물번호')\n","# test_df = pd.merge(test_df, infos[['건물번호', '건물유형', '연면적(m2)', '냉방면적(m2)']], how = 'outer',on = '건물번호')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1041,"status":"ok","timestamp":1693037083410,"user":{"displayName":"박여오","userId":"10243965647474797689"},"user_tz":-540},"id":"HvOGyAF9fmZU","outputId":"894315e8-7826-43ce-edac-065eeb7742a4"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18256\\1945127973.py:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n","  train_df['week'] = date.dt.weekofyear\n","C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18256\\1945127973.py:11: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n","  test_df['week'] = date.dt.weekofyear\n"]}],"source":["date = pd.to_datetime(train_df.일시)\n","train_df['hour'] = date.dt.hour\n","train_df['day'] = date.dt.weekday\n","train_df['month'] = date.dt.month\n","train_df['week'] = date.dt.weekofyear\n","\n","date = pd.to_datetime(test_df.일시)\n","test_df['hour'] = date.dt.hour\n","test_df['day'] = date.dt.weekday\n","test_df['month'] = date.dt.month\n","test_df['week'] = date.dt.weekofyear\n","\n","# train_df[['강수량(mm)', '풍속(m/s)', '습도(%)']] = train_df.groupby(['month', '건물번호'])[['강수량(mm)', '풍속(m/s)', '습도(%)']].transform(lambda x: x.fillna(x.mean()))\n","\n","for month in train_df['month'].unique():\n","    train_df.loc[train_df['month'] == month, ['강수량(mm)', '풍속(m/s)', '습도(%)']] = train_df.loc[train_df['month'] == month, ['강수량(mm)', '풍속(m/s)', '습도(%)']].fillna(train_df.loc[train_df['month'] == month, ['강수량(mm)', '풍속(m/s)', '습도(%)']].mean())\n","# for month in train_df['month'].unique():\n","#     train_df.loc[train_df['month'] == month, ['풍속(m/s)', '습도(%)']] = train_df.loc[train_df['month'] == month, ['풍속(m/s)', '습도(%)']].fillna(train_df.loc[train_df['month'] == month, ['풍속(m/s)', '습도(%)']].mean())"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"H-ii0oKIfmZU"},"outputs":[],"source":["train_df = train_df.fillna(0)\n","test_df = test_df.fillna(0)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"R-92SaenfmZV"},"outputs":[],"source":["train_df.loc[(train_df['건물번호'] == 95) & (train_df['전력소비량(kWh)'] < 1), '전력소비량(kWh)'] = np.NaN\n","train_df['전력소비량(kWh)'] = train_df['전력소비량(kWh)'].interpolate(limit_direction='both', method='linear').round(3)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ib2Z-0nsfmZV"},"outputs":[],"source":["# # 1\n","# train_df.loc[770:777, '전력소비량(kWh)'] = train_df.loc[602:609, '전력소비량(kWh)'].values\n","# train_df.loc[939:946, '전력소비량(kWh)'] = train_df.loc[602:609, '전력소비량(kWh)'].values\n","\n","# # 4\n","# train_df.loc[7438:7439, '전력소비량(kWh)'] = train_df.loc[7416:7417, '전력소비량(kWh)'].values\n","# train_df.loc[7440, '전력소비량(kWh)'] = np.nan\n","\n","# # 4\n","# # train_df.loc[7438, '전력소비량(kWh)'] = train_df.loc[7438-24*7, '전력소비량(kWh)'].values\n","\n","# # 11\n","# train_df.loc[21893, '전력소비량(kWh)'] = train_df.loc[21893-24*7, '전력소비량(kWh)']\n","# train_df.loc[22058, '전력소비량(kWh)'] = train_df.loc[22058-24*7, '전력소비량(kWh)']\n","# train_df.loc[22065, '전력소비량(kWh)'] = train_df.loc[22065-24*7, '전력소비량(kWh)']\n","# train_df.loc[21700:21701, '전력소비량(kWh)'] = train_df.loc[21700-24*7:21701-24*7, '전력소비량(kWh)'].values\n","# train_df.loc[22243, '전력소비량(kWh)'] = train_df.loc[22243-24*7, '전력소비량(kWh)']\n","\n","# # 17\n","# train_df.loc[33888:33901, '전력소비량(kWh)'] = train_df.loc[33888-24*7:33901-24*7, '전력소비량(kWh)'].values\n","\n","# # 26\n","# train_df.loc[52089:52090, '전력소비량(kWh)'] = train_df.loc[52089-24*7:52090-24*7, '전력소비량(kWh)'].values\n","\n","# # 28\n","# train_df.loc[56450, '전력소비량(kWh)'] = train_df.loc[56450-24*7, '전력소비량(kWh)']\n","\n","# # 31\n","# train_df.loc[61955, '전력소비량(kWh)'] = train_df.loc[61955-24*7, '전력소비량(kWh)']\n","\n","# # 34\n","# train_df.loc[68973, '전력소비량(kWh)'] = train_df.loc[68973-24*7, '전력소비량(kWh)']\n","\n","# # 56\n","# train_df.loc[112384, '전력소비량(kWh)'] = train_df.loc[112384-24*7, '전력소비량(kWh)']\n","\n","# # 61\n","# train_df.loc[123132, '전력소비량(kWh)'] = train_df.loc[123132-24*7, '전력소비량(kWh)']\n","\n","# # 69\n","# train_df.loc[138904, '전력소비량(kWh)'] = train_df.loc[138904-24*7, '전력소비량(kWh)']\n","\n","# # 72\n","# train_df.loc[145424:145425, '전력소비량(kWh)'] = np.nan\n","# train_df.loc[145786, '전력소비량(kWh)'] = np.nan\n","# train_df.loc[145954, '전력소비량(kWh)'] = np.nan\n","\n","# # 95\n","# train_df.loc[193120:193121, '전력소비량(kWh)'] = train_df.loc[193120-24*7:193121-24*7, '전력소비량(kWh)'].values\n","# train_df.loc[193133, '전력소비량(kWh)'] = train_df.loc[193133-24*7, '전력소비량(kWh)']\n","\n","# # 75\n","# train_df.loc[151303:151314, '전력소비량(kWh)'] = train_df.loc[151303-24*7:151314-24*7, '전력소비량(kWh)'].values\n","# train_df.loc[151423:151455, '전력소비량(kWh)'] = train_df.loc[151423-24*7:151455-24*7, '전력소비량(kWh)'].values\n","# train_df.loc[152393, '전력소비량(kWh)'] = train_df.loc[152393-24*7, '전력소비량(kWh)']\n","\n","# # 70\n","# # train.loc[142200:142229, '전력소비량(kWh)'] = np.nan\n","# train_df.loc[142004+169:142004+169+168, '전력소비량(kWh)'] = train_df.loc[142004:142004+168, '전력소비량(kWh)'].values\n","# train_df.loc[142171:142172, '전력소비량(kWh)'] = train_df.loc[142171-24*7:142172-24*7, '전력소비량(kWh)'].values\n","# train_df.loc[142340:142379, '전력소비량(kWh)'] = train_df.loc[142340-24*7:142379-24*7, '전력소비량(kWh)'].values\n","# # train.loc[142200:142229, '전력소비량(kWh)'] = train.loc[142200-24*7:142229-24*7, '전력소비량(kWh)'].values\n","\n","# train_df['전력소비량(kWh)'] = train_df['전력소비량(kWh)'].interpolate(limit_direction='both', method='linear').round(3)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"O6Tb0UPmfmZW"},"outputs":[],"source":["power_mean = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물번호', 'day', 'month'], aggfunc = np.mean).reset_index()\n","train_df['month_day_mean'] = train_df.apply(lambda x : power_mean.loc[(power_mean.건물번호 == x['건물번호']) & (power_mean.day == x['day']) & (power_mean.month == x['month']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['month_day_mean'] = test_df.apply(lambda x : power_mean.loc[(power_mean.건물번호 == x['건물번호']) & (power_mean.day == x['day']) & (power_mean.month == x['month']) ,'전력소비량(kWh)'].values[0], axis = 1)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"RbLOIdcJfmZW"},"outputs":[],"source":["power_mean = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물번호', 'hour', 'day'], aggfunc = np.mean).reset_index()\n","train_df['day_hour_mean'] = train_df.apply(lambda x : power_mean.loc[(power_mean.건물번호 == x['건물번호']) & (power_mean.hour == x['hour']) & (power_mean.day == x['day']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['day_hour_mean'] = test_df.apply(lambda x : power_mean.loc[(power_mean.건물번호 == x['건물번호']) & (power_mean.hour == x['hour']) & (power_mean.day == x['day']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","\n","power_std = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물번호', 'hour', 'day'], aggfunc = np.std).reset_index()\n","train_df['day_hour_std'] = train_df.apply(lambda x : power_std.loc[(power_std.건물번호 == x['건물번호']) & (power_std.hour == x['hour']) & (power_std.day == x['day']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['day_hour_std'] = test_df.apply(lambda x : power_std.loc[(power_std.건물번호 == x['건물번호']) & (power_std.hour == x['hour']) & (power_std.day == x['day']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","\n","power_hour_mean = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물번호', 'hour'], aggfunc = np.mean).reset_index()\n","train_df['hour_mean'] = train_df.apply(lambda x : power_hour_mean.loc[(power_hour_mean.건물번호 == x['건물번호']) & (power_hour_mean.hour == x['hour']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['hour_mean'] = test_df.apply(lambda x : power_hour_mean.loc[(power_hour_mean.건물번호 == x['건물번호']) & (power_hour_mean.hour == x['hour']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","\n","power_hour_std = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물번호', 'hour'], aggfunc = np.std).reset_index()\n","train_df['hour_std'] = train_df.apply(lambda x : power_hour_std.loc[(power_hour_std.건물번호 == x['건물번호']) & (power_hour_std.hour == x['hour']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['hour_std'] = test_df.apply(lambda x : power_hour_std.loc[(power_hour_std.건물번호 == x['건물번호']) & (power_hour_std.hour == x['hour']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","\n","\n","power_mean = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물유형', 'hour', 'day'], aggfunc = np.mean).reset_index()\n","train_df['type_day_hour_mean'] = train_df.apply(lambda x : power_mean.loc[(power_mean.건물유형 == x['건물유형']) & (power_mean.hour == x['hour']) & (power_mean.day == x['day']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['type_day_hour_mean'] = test_df.apply(lambda x : power_mean.loc[(power_mean.건물유형 == x['건물유형']) & (power_mean.hour == x['hour']) & (power_mean.day == x['day']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","\n","power_std = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물유형', 'hour', 'day'], aggfunc = np.std).reset_index()\n","train_df['type_day_hour_std'] = train_df.apply(lambda x : power_std.loc[(power_std.건물유형 == x['건물유형']) & (power_std.hour == x['hour']) & (power_std.day == x['day']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['type_day_hour_std'] = test_df.apply(lambda x : power_std.loc[(power_std.건물유형 == x['건물유형']) & (power_std.hour == x['hour']) & (power_std.day == x['day']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","\n","power_hour_mean = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물유형', 'hour'], aggfunc = np.mean).reset_index()\n","train_df['type_hour_mean'] = train_df.apply(lambda x : power_hour_mean.loc[(power_hour_mean.건물유형 == x['건물유형']) & (power_hour_mean.hour == x['hour']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['type_hour_mean'] = test_df.apply(lambda x : power_hour_mean.loc[(power_hour_mean.건물유형 == x['건물유형']) & (power_hour_mean.hour == x['hour']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","\n","power_hour_std = pd.pivot_table(train_df, values = '전력소비량(kWh)', index = ['건물유형', 'hour'], aggfunc = np.std).reset_index()\n","train_df['type_hour_std'] = train_df.apply(lambda x : power_hour_std.loc[(power_hour_std.건물유형 == x['건물유형']) & (power_hour_std.hour == x['hour']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","test_df['type_hour_std'] = test_df.apply(lambda x : power_hour_std.loc[(power_hour_std.건물유형 == x['건물유형']) & (power_hour_std.hour == x['hour']) ,'전력소비량(kWh)'].values[0], axis = 1)\n","\n","\n","### 공휴일 변수 추가\n","train_df['holiday'] = train_df.apply(lambda x : 0 if x['day']<5 else 1, axis = 1)\n","train_df.loc[('20220601' <= train_df.일시)&(train_df.일시 < '20220602'), 'holiday'] = 1\n","train_df.loc[('20220606' <= train_df.일시)&(train_df.일시 < '20220607'), 'holiday'] = 1\n","train_df.loc[('20220815' <= train_df.일시)&(train_df.일시 < '20220816'), 'holiday'] = 1\n","\n","test_df['holiday'] = test_df.apply(lambda x : 0 if x['day']<5 else 1, axis = 1)\n","\n","train_df['sin_time'] = np.sin(2*np.pi*train_df.hour/24)\n","train_df['cos_time'] = np.cos(2*np.pi*train_df.hour/24)\n","test_df['sin_time'] = np.sin(2*np.pi*test_df.hour/24)\n","test_df['cos_time'] = np.cos(2*np.pi*test_df.hour/24)\n","\n","train_df['THI'] = 9/5*train_df['기온(C)'] - 0.55*(1-train_df['습도(%)']/100)*(9/5*train_df['습도(%)']-26)+32\n","test_df['THI'] = 9/5*test_df['기온(C)'] - 0.55*(1-test_df['습도(%)']/100)*(9/5*test_df['습도(%)']-26)+32\n","\n","def CDH(xs):\n","    ys = []\n","    for i in range(len(xs)):\n","        if i < 11:\n","            ys.append(np.sum(xs[:(i+1)]-26))\n","        else:\n","            ys.append(np.sum(xs[(i-11):(i+1)]-26))\n","    return np.array(ys)\n","\n","train_df['CDH'] = 0\n","for num in range(1,101,1):\n","    temp = train_df[train_df['건물번호'] == num]\n","    cdh = CDH(temp['기온(C)'].values)\n","    train_df.loc[train_df['건물번호'] == num, 'CDH'] = cdh\n","\n","test_df['CDH'] = 0\n","for num in range(1,101,1):\n","    temp = test_df[test_df['건물번호'] == num]\n","    cdh = CDH(temp['기온(C)'].values)\n","    test_df.loc[test_df['건물번호'] == num, 'CDH'] = cdh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72oIJhVffmZX"},"outputs":[],"source":["train_df['7_shifted_전력소비량'] = train_df['전력소비량(kWh)'].shift(24*7)\n","train_df = train_df[train_df.일시 >= '20220608'].reset_index(drop=True)\n","\n","for i in train_df['건물번호'].unique():\n","    test_df.loc[test_df['건물번호'] == i, '7_shifted_전력소비량'] = train_df.loc[train_df['건물번호'] == i, '전력소비량(kWh)'][-7*24:].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWzIGs3zfmZX"},"outputs":[],"source":["# # 화씨온도\n","# train_df['temperature_F'] = (train_df['기온(C)'] * 9/5) + 32\n","# test_df['temperature_F'] = (test_df['기온(C)'] * 9/5) + 32\n","\n","# 둘째, 넷째 주 일요일\n","# train_df['2_4_sunday'] = 0\n","# train_df.loc[('20220612' <= train_df.일시)&(train_df.일시 < '20220613'), '2_4_sunday'] = 1\n","# train_df.loc[('20220626' <= train_df.일시)&(train_df.일시 < '20220617'), '2_4_sunday'] = 1\n","# train_df.loc[('20220710' <= train_df.일시)&(train_df.일시 < '20220711'), '2_4_sunday'] = 1\n","# train_df.loc[('20220724' <= train_df.일시)&(train_df.일시 < '20220725'), '2_4_sunday'] = 1\n","# train_df.loc[('20220814' <= train_df.일시)&(train_df.일시 < '20220815'), '2_4_sunday'] = 1\n","\n","# test_df['2_4_sunday'] = 0\n","# test_df.loc[('20220828' <= test_df.일시)&(test_df.일시 < '20220829'), '2_4_sunday'] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1693038480299,"user":{"displayName":"박여오","userId":"10243965647474797689"},"user_tz":-540},"id":"3dIFPtmxfmZY","outputId":"c7242736-3945-4771-f5de-0e6f6944dfe3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>건물번호</th>\n","      <th>기온(C)</th>\n","      <th>강수량(mm)</th>\n","      <th>풍속(m/s)</th>\n","      <th>습도(%)</th>\n","      <th>태양광용량(kW)</th>\n","      <th>hour</th>\n","      <th>day</th>\n","      <th>month</th>\n","      <th>week</th>\n","      <th>...</th>\n","      <th>type_day_hour_mean</th>\n","      <th>type_day_hour_std</th>\n","      <th>type_hour_mean</th>\n","      <th>type_hour_std</th>\n","      <th>holiday</th>\n","      <th>sin_time</th>\n","      <th>cos_time</th>\n","      <th>THI</th>\n","      <th>CDH</th>\n","      <th>7_shifted_전력소비량</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>19.0</td>\n","      <td>1.889045</td>\n","      <td>3.2</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>23</td>\n","      <td>...</td>\n","      <td>1620.812462</td>\n","      <td>886.546805</td>\n","      <td>1616.129012</td>\n","      <td>909.159339</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>48.2249</td>\n","      <td>-38.9</td>\n","      <td>1085.28</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>18.8</td>\n","      <td>1.889045</td>\n","      <td>2.6</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>23</td>\n","      <td>...</td>\n","      <td>1608.544000</td>\n","      <td>906.029700</td>\n","      <td>1603.843635</td>\n","      <td>921.827876</td>\n","      <td>0</td>\n","      <td>0.258819</td>\n","      <td>0.965926</td>\n","      <td>47.8649</td>\n","      <td>-45.1</td>\n","      <td>1047.36</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>18.5</td>\n","      <td>1.889045</td>\n","      <td>2.6</td>\n","      <td>62.0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>23</td>\n","      <td>...</td>\n","      <td>1577.389538</td>\n","      <td>900.729730</td>\n","      <td>1575.108376</td>\n","      <td>916.043890</td>\n","      <td>0</td>\n","      <td>0.500000</td>\n","      <td>0.866025</td>\n","      <td>47.4096</td>\n","      <td>-51.4</td>\n","      <td>974.88</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>18.1</td>\n","      <td>1.889045</td>\n","      <td>2.2</td>\n","      <td>63.0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>23</td>\n","      <td>...</td>\n","      <td>1561.107077</td>\n","      <td>900.693834</td>\n","      <td>1553.335094</td>\n","      <td>908.635022</td>\n","      <td>0</td>\n","      <td>0.707107</td>\n","      <td>0.707107</td>\n","      <td>46.7941</td>\n","      <td>-58.0</td>\n","      <td>953.76</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>17.2</td>\n","      <td>1.889045</td>\n","      <td>3.2</td>\n","      <td>66.0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>23</td>\n","      <td>...</td>\n","      <td>1546.894154</td>\n","      <td>885.287208</td>\n","      <td>1543.278965</td>\n","      <td>900.520543</td>\n","      <td>0</td>\n","      <td>0.866025</td>\n","      <td>0.500000</td>\n","      <td>45.6064</td>\n","      <td>-64.9</td>\n","      <td>986.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 25 columns</p>\n","</div>"],"text/plain":["   건물번호  기온(C)   강수량(mm)  풍속(m/s)  습도(%)  태양광용량(kW)  hour  day  month  week  \\\n","0     1   19.0  1.889045      3.2   61.0          0     0    2      6    23   \n","1     1   18.8  1.889045      2.6   61.0          0     1    2      6    23   \n","2     1   18.5  1.889045      2.6   62.0          0     2    2      6    23   \n","3     1   18.1  1.889045      2.2   63.0          0     3    2      6    23   \n","4     1   17.2  1.889045      3.2   66.0          0     4    2      6    23   \n","\n","   ...  type_day_hour_mean  type_day_hour_std  type_hour_mean  type_hour_std  \\\n","0  ...         1620.812462         886.546805     1616.129012     909.159339   \n","1  ...         1608.544000         906.029700     1603.843635     921.827876   \n","2  ...         1577.389538         900.729730     1575.108376     916.043890   \n","3  ...         1561.107077         900.693834     1553.335094     908.635022   \n","4  ...         1546.894154         885.287208     1543.278965     900.520543   \n","\n","   holiday  sin_time  cos_time      THI   CDH  7_shifted_전력소비량  \n","0        0  0.000000  1.000000  48.2249 -38.9          1085.28  \n","1        0  0.258819  0.965926  47.8649 -45.1          1047.36  \n","2        0  0.500000  0.866025  47.4096 -51.4           974.88  \n","3        0  0.707107  0.707107  46.7941 -58.0           953.76  \n","4        0  0.866025  0.500000  45.6064 -64.9           986.40  \n","\n","[5 rows x 25 columns]"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["train_x = train_df.drop(columns=['num_date_time', '일시', '일조(hr)', '일사(MJ/m2)', '전력소비량(kWh)', '건물유형'])\n","train_x_type = train_df.drop(columns=['num_date_time', '일시', '일조(hr)', '일사(MJ/m2)', '전력소비량(kWh)'])\n","train_y = train_df['전력소비량(kWh)']\n","\n","test_x = test_df[train_x.columns]\n","test_x_type = test_df[train_x_type.columns]\n","train_x.head()"]},{"cell_type":"markdown","metadata":{"id":"O4ZaP3HqfmZY"},"source":["## modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hisEZ1lbfmZY"},"outputs":[],"source":["# custom objective function for forcing model not to underestimate\n","def weighted_mse(alpha = 1):\n","    def weighted_mse_fixed(label, pred):\n","        residual = (label - pred).astype(\"float\")\n","        grad = np.where(residual>0, -2*alpha*residual, -2*residual)\n","        hess = np.where(residual>0, 2*alpha, 2.0)\n","        return grad, hess\n","    return weighted_mse_fixed\n","\n","def SMAPE(true, pred):\n","    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 100\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbntI16YfmZY"},"outputs":[],"source":["# train_x_type[\"hour\"] = train_x_type[\"hour\"].astype(\"category\")\n","# test_x_type[\"hour\"] = test_x_type[\"hour\"].astype(\"category\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mape_custom = make_scorer(name='SMAPE',\n","                                 score_func=SMAPE,\n","                                 optimum=0,\n","                                 greater_is_better=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnbOHA_Wfzhj"},"outputs":[{"data":{"text/plain":["range(15, 16)"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["alphas = range(15, 16)\n","alphas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130643\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130643\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.08 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.530014644176218, 6.674005681880703, 7.81407, 0.44399)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n"]},{"name":"stderr","output_type":"stream","text":["Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7741.67 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2805\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00372338\tvalid_set's SMAPE: -1.75431\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3327\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2944\t = Validation score   (-SMAPE)\n","\t1.01s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2695\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.2s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130643\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130645\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130645\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.08 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.530014644176218, 6.652553303974038, 7.81862, 0.44228)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7713.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3097\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3557\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.433\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3076\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.77s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130645\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130647\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130647\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.08 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.530014644176218, 6.652553303974038, 7.81748, 0.43389)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7709.14 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3321\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3408\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4107\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3265\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.75s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130647\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130649\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130649\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.07 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.508701236615565, 6.652553303974038, 7.82517, 0.43506)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7716.15 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2989\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.298\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5098\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2888\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.92s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130649\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130651\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130651\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.07 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.530014644176218, 6.652553303974038, 7.80317, 0.44253)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7722.4 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.269\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3038\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4094\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2676\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.6s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130651\\\")\n","  1%|          | 1/100 [00:09<16:02,  9.73s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130652\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130652\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.07 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.013118065020423, 6.189003069706797, 7.37896, 0.36226)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7719.25 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4039\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4089\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4431\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3944\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.27s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130652\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130655\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130655\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.06 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.024141995749412, 6.189003069706797, 7.3852, 0.36733)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7721.25 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4177\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4256\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.562\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4113\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.87s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130655\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130657\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130657\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.06 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.024141995749412, 6.189003069706797, 7.37882, 0.36463)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7719.7 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4185\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4379\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5766\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.411\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.91s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130657\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130659\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130659\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.05 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.024141995749412, 6.268490039371234, 7.38589, 0.36469)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7717.7 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.1s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3996\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4003\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4507\t = Validation score   (-SMAPE)\n","\t0.86s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3906\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.21s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130659\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130701\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130701\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.04 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.024141995749412, 6.189003069706797, 7.36933, 0.36368)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7712.1 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4165\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00817329\tvalid_set's SMAPE: -1.49427\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.4529\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5152\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4158\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.15s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130701\\\")\n","  2%|▏         | 2/100 [00:20<17:03, 10.45s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130703\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130703\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.04 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.22673393281716, 6.586116480918828, 7.20936, 0.39372)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7710.52 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4958\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4743\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5309\t = Validation score   (-SMAPE)\n","\t0.82s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4717\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.83s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130703\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130705\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130705\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.04 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.22673393281716, 6.586116480918828, 7.21815, 0.39599)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7707.17 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4545\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4442\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4809\t = Validation score   (-SMAPE)\n","\t0.93s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4308\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.28s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130705\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130708\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130708\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.03 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.22673393281716, 6.586116480918828, 7.20655, 0.39426)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7719.4 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.461\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.519\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5537\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4597\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.99s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130708\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130710\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130710\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.02 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.22673393281716, 6.588100827287083, 7.21703, 0.39092)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7703.17 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.424\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4562\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4743\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4199\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.08s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130710\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130712\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130712\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.02 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.223165695949474, 6.586116480918828, 7.20611, 0.39476)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7713.84 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4477\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00987074\tvalid_set's SMAPE: -1.65013\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.4895\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5923\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4461\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.12s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130712\\\")\n","  3%|▎         | 3/100 [00:31<17:10, 10.63s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130714\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130714\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.01 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.394788085220735, 6.10022926189937, 6.87266, 0.28592)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7698.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.282\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2913\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3131\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2813\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.68s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130714\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130716\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130716\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.01 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.406006171165975, 6.10022926189937, 6.87448, 0.28484)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7699.11 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.8146\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.8126\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.849\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.8126\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.43s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130716\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130718\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130718\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.01 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.406006171165975, 6.10022926189937, 6.87217, 0.28512)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7712.25 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.8022\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2536\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.8378\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2516\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.51s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130718\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130719\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130719\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.01 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.406006171165975, 6.10022926189937, 6.8792, 0.28516)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7715.6 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2467\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0017291\tvalid_set's SMAPE: -2.08029\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2666\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3406\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2458\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.93s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130719\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130721\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130721\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.00 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.406006171165975, 6.364991691680206, 6.86514, 0.28467)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7697.22 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.267\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2832\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2886\t = Validation score   (-SMAPE)\n","\t0.81s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2653\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.71s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130721\\\")\n","  4%|▍         | 4/100 [00:40<15:44,  9.84s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130723\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130723\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.00 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.948409213704661, 6.939990109115495, 7.78447, 0.48463)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7695.44 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3368\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3243\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4292\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3147\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.9s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130723\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130725\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130725\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   126.00 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.948409213704661, 6.939990109115495, 7.7954, 0.49323)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7705.75 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.332\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3384\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3513\t = Validation score   (-SMAPE)\n","\t0.82s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3096\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.87s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130725\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130727\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130727\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.99 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.947004835977172, 6.939990109115495, 7.8025, 0.49021)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7715.79 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.278\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.304\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.306\t = Validation score   (-SMAPE)\n","\t1.12s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2727\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130727\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130729\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130729\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.99 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.948409213704661, 6.975600825877204, 7.79788, 0.49271)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7698.6 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00477517\tvalid_set's SMAPE: -2.16177\n","[2000]\tvalid_set's l2: 0.00420617\tvalid_set's SMAPE: -2.15148\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2733\t = Validation score   (-SMAPE)\n","\t1.12s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3375\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3115\t = Validation score   (-SMAPE)\n","\t1.02s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2714\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 3.05s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130729\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130733\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130733\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.97 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.948409213704661, 6.939990109115495, 7.78409, 0.48298)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7684.58 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00599172\tvalid_set's SMAPE: -2.09224\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2838\t = Validation score   (-SMAPE)\n","\t0.95s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3187\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3166\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2765\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.58s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130733\\\")\n","  5%|▌         | 5/100 [00:52<17:04, 10.79s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130735\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130735\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.97 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.12753450109065, 6.784683281392143, 7.51602, 0.42082)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7681.53 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.22\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1933\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.2087\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1933\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.6s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130735\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130737\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130737\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.96 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.12753450109065, 6.771568936523823, 7.5198, 0.42155)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7695.33 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2223\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2012\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4275\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1992\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.64s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130737\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130739\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130739\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.96 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.12753450109065, 6.771568936523823, 7.51429, 0.41899)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7710.35 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.7757\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2108\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.7556\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2108\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130739\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130740\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130740\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.96 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.121278334139902, 6.771568936523823, 7.53033, 0.41552)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7707.46 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1662\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00112678\tvalid_set's SMAPE: -1.12958\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.1842\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2009\t = Validation score   (-SMAPE)\n","\t0.82s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1656\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.03s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130740\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130742\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130742\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.96 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.12753450109065, 6.771568936523823, 7.50467, 0.42265)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7705.28 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1752\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00151998\tvalid_set's SMAPE: -1.13133\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2054\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2193\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1752\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.0s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130742\\\")\n","  6%|▌         | 6/100 [01:01<16:01, 10.22s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130745\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130745\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.95 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.2611970016897525, 5.666426688112432, 6.55927, 0.44429)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7735.4 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.381\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3753\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.1468\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3651\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.92s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130745\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130747\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130747\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.95 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.321056272858076, 5.700711096131715, 6.56989, 0.43818)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7739.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4547\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.0582\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.291\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4547\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.58s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130747\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130748\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130748\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.95 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.321056272858076, 5.666426688112432, 6.5617, 0.44364)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7735.88 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3682\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3785\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5291\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.356\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.76s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130748\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130750\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130750\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.94 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.321056272858076, 5.666426688112432, 6.569, 0.44293)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7732.75 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4053\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4342\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9022\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4034\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.74s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130750\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130752\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130752\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.94 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.321056272858076, 5.666426688112432, 6.54759, 0.44715)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7729.02 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3705\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4544\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4125\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3586\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.78s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130752\\\")\n","  7%|▋         | 7/100 [01:10<15:19,  9.88s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130754\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130754\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.94 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.506437922081462, 6.733163768250093, 7.19329, 0.13321)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7724.27 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7464\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7401\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7495\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7401\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130754\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130755\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130755\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.94 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.506437922081462, 6.733163768250093, 7.19084, 0.1351)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7719.78 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7101\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7013\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.713\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7013\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130755\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130757\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130757\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.93 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.506437922081462, 6.740282925038388, 7.19089, 0.13484)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7726.2 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7053\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7024\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7127\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7024\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.37s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130757\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130758\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130758\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.93 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.496097345175956, 6.733163768250093, 7.19106, 0.13351)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7720.42 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7486\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7445\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7565\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7445\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.42s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130758\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130800\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130800\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.93 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.506437922081462, 6.733163768250093, 7.19087, 0.13395)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7728.34 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7446\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7432\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.755\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7432\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.37s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130800\\\")\n","  8%|▊         | 8/100 [01:18<13:53,  9.06s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130801\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130801\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.93 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.27034079345522, 6.836001456718747, 7.69654, 0.4048)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7721.69 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2132\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1665\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2069\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.164\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.75s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130801\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130803\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130803\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.93 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.27034079345522, 6.836001456718747, 7.70262, 0.40662)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7723.03 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1784\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1725\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4661\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1653\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.7s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130803\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130805\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130805\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.93 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.27034079345522, 6.836001456718747, 7.69542, 0.40463)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7726.35 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3553\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1597\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.301\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1596\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.58s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130805\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130806\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130806\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.93 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.27034079345522, 6.838062162380792, 7.71101, 0.40048)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7722.37 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1761\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1652\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1739\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1549\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.8s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130806\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130808\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130808\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.92 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.26170205281182, 6.836001456718747, 7.68688, 0.40709)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7719.09 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2013\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3007\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1694\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.168\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.78s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130808\\\")\n","  9%|▉         | 9/100 [01:27<13:44,  9.06s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130810\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130810\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.92 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.635687085464026, 7.806175276253138, 8.31678, 0.13346)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7717.74 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1914\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1802\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2002\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1785\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.9s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130810\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130812\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130812\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.91 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.638156941012365, 7.806175276253138, 8.31827, 0.13209)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7707.82 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1786\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1903\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1954\t = Validation score   (-SMAPE)\n","\t0.82s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1755\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.05s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130812\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130814\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130814\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.91 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.638156941012365, 7.810271577434452, 8.31339, 0.13385)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7741.39 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1586\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00112901\tvalid_set's SMAPE: -2.01566\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.1726\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1842\t = Validation score   (-SMAPE)\n","\t0.89s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.158\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.34s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130814\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130817\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130817\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.90 GB / 498.62 GB (25.3%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.638156941012365, 7.806175276253138, 8.31804, 0.13403)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7724.32 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1902\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1916\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1936\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1849\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.72s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130817\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130818\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130818\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.90 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.638156941012365, 7.806175276253138, 8.31645, 0.13298)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7719.38 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1735\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0014357\tvalid_set's SMAPE: -1.92635\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.1852\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1961\t = Validation score   (-SMAPE)\n","\t0.96s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1727\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.3s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130818\\\")\n"," 10%|█         | 10/100 [01:38<14:24,  9.61s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130821\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130821\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.89 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.8840793991050715, 6.858943114570701, 7.56342, 0.159)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8022.51 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1761\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1695\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2713\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1685\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130821\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130823\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130823\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.89 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.8840793991050715, 6.858943114570701, 7.56229, 0.15969)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8031.15 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3294\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2074\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2053\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2034\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130823\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130824\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130824\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.89 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.880275104122722, 6.858943114570701, 7.56413, 0.15771)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8031.69 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1573\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1694\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2156\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1546\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.13s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130824\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130827\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130827\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.88 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.8840793991050715, 7.192843991581319, 7.56549, 0.15501)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8035.26 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3535\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3934\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3959\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3535\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.4s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130827\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130828\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130828\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.88 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.8840793991050715, 6.858943114570701, 7.5593, 0.15902)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8040.15 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.165\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00136838\tvalid_set's SMAPE: -2.38913\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.1712\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1865\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1623\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.02s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130828\\\")\n"," 11%|█         | 11/100 [01:47<14:07,  9.52s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130830\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130830\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.88 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.686042920793153, 6.850338008112186, 7.27216, 0.18871)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8017.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.17\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1909\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2187\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1697\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.94s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130830\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130832\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130832\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.87 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.686042920793153, 6.814630708991662, 7.27367, 0.18866)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8021.65 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2211\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.19\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.194\t = Validation score   (-SMAPE)\n","\t1.06s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1866\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.0s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130832\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130834\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130834\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.87 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.686042920793153, 6.814630708991662, 7.27158, 0.18628)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8020.38 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1918\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2003\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2003\t = Validation score   (-SMAPE)\n","\t0.89s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1867\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.88s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130834\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130836\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130836\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.86 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.686042920793153, 6.814630708991662, 7.27605, 0.18635)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8026.0 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1542\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1849\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2073\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1542\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.95s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130836\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130838\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130838\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.86 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.668869471681888, 6.814630708991662, 7.26652, 0.18743)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8011.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1606\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1742\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1956\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1584\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.94s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130838\\\")\n"," 12%|█▏        | 12/100 [01:57<14:14,  9.71s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130840\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130840\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.85 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.125832151674345, 7.241624129733235, 7.8435, 0.13023)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8015.76 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6444\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.6401\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2706\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2706\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.72s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130840\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130842\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130842\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.85 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.125832151674345, 7.344744893609112, 7.84588, 0.12934)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8024.64 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2152\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2619\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3143\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2151\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.81s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130842\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130844\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130844\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.85 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.125832151674345, 7.241624129733235, 7.84199, 0.1302)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8024.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6313\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2862\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.272\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2719\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.66s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130844\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130846\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130846\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.84 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.125832151674345, 7.241624129733235, 7.84534, 0.12934)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8017.95 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2874\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.254\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2835\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.254\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.78s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130846\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130848\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130848\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.84 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.120136632166567, 7.241624129733235, 7.84738, 0.13124)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8006.23 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6322\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3119\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3479\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3119\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130848\\\")\n"," 13%|█▎        | 13/100 [02:06<13:43,  9.46s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130849\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130849\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.84 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.03228219750093, 6.6956011224675285, 7.52827, 0.25274)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8012.37 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6711\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7223\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7742\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.671\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.83s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130849\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130851\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130851\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.84 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.03228219750093, 6.727144258386629, 7.53075, 0.2482)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8003.3 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0183471\tvalid_set's SMAPE: -2.16\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.7143\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.773\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8086\t = Validation score   (-SMAPE)\n","\t0.98s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7143\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.66s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130851\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130854\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130854\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.83 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.9703810853958394, 6.6956011224675285, 7.52554, 0.24965)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7983.76 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0170916\tvalid_set's SMAPE: -2.14483\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.6674\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0194839\tvalid_set's SMAPE: -2.19045\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.7062\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7866\t = Validation score   (-SMAPE)\n","\t1.23s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.6666\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 3.04s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130854\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130857\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130857\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.81 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.03228219750093, 6.6956011224675285, 7.52733, 0.24966)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.84 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.016918\tvalid_set's SMAPE: -2.05885\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.6898\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.756\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8443\t = Validation score   (-SMAPE)\n","\t1.07s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.6898\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.71s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130857\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130900\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130900\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.80 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.03228219750093, 6.6956011224675285, 7.53045, 0.24774)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7976.99 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7088\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0183329\tvalid_set's SMAPE: -2.16367\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.7389\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7965\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7038\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.31s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130900\\\")\n"," 14%|█▍        | 14/100 [02:19<15:09, 10.58s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130902\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130902\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.80 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.754249959764082, 7.013474730214882, 7.48529, 0.13587)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7985.13 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1337\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1456\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1602\t = Validation score   (-SMAPE)\n","\t0.9s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1331\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.11s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130902\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130905\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130905\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.79 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.743889630055544, 7.013474730214882, 7.48683, 0.13468)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.76 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.144\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.15\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1529\t = Validation score   (-SMAPE)\n","\t0.99s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.14\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.03s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130905\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130907\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130907\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.79 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.754249959764082, 7.03510133177154, 7.48485, 0.1357)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7987.46 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1287\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1435\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1525\t = Validation score   (-SMAPE)\n","\t1.18s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1285\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.3s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130907\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130909\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130909\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.78 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.754249959764082, 7.013474730214882, 7.48732, 0.13586)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7998.54 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1489\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1555\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1708\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1474\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.01s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130909\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130911\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130911\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.77 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.754249959764082, 7.013474730214882, 7.4877, 0.13653)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7990.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.000626021\tvalid_set's SMAPE: -2.64018\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.1223\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1439\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1573\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1221\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.25s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130911\\\")\n"," 15%|█▌        | 15/100 [02:30<15:15, 10.77s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130914\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130914\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.77 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.585053688676094, 6.857934583998181, 8.02483, 0.54169)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7987.23 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2173\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2282\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2304\t = Validation score   (-SMAPE)\n","\t1.07s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2135\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.26s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130914\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130916\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130916\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.76 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.585053688676094, 6.995802785670617, 8.03954, 0.53522)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7986.42 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2144\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2333\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2326\t = Validation score   (-SMAPE)\n","\t0.94s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2084\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.92s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130916\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130918\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130918\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.76 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.585053688676094, 6.857934583998181, 8.03466, 0.53728)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7998.25 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1917\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00198447\tvalid_set's SMAPE: -1.926\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2074\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2173\t = Validation score   (-SMAPE)\n","\t1.05s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.191\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.5s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130918\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130921\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130921\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.75 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.585053688676094, 6.857934583998181, 8.03842, 0.53556)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7974.25 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2104\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00244237\tvalid_set's SMAPE: -1.90746\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2177\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00293126\tvalid_set's SMAPE: -1.90549\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2548\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2069\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130921\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130923\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130923\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.74 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.577851695011619, 6.857934583998181, 8.01066, 0.54595)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7979.51 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1868\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0018291\tvalid_set's SMAPE: -2.06791\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2077\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2013\t = Validation score   (-SMAPE)\n","\t1.02s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1822\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.4s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130923\\\")\n"," 16%|█▌        | 16/100 [02:42<15:38, 11.17s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130926\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130926\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.73 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.523016590884787, 5.693462742249347, 6.80798, 0.44259)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7968.38 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3365\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3114\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2574\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2561\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.78s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130926\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130928\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130928\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.73 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.523016590884787, 5.693462742249347, 6.81196, 0.44092)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7986.48 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.283\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3755\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3194\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2718\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.9s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130928\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130930\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130930\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.73 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.523016590884787, 5.69588470252906, 6.8126, 0.44036)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7997.35 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3166\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2903\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2685\t = Validation score   (-SMAPE)\n","\t0.89s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2627\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.06s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130930\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130932\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130932\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.72 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.523016590884787, 5.693462742249347, 6.81103, 0.43891)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7998.95 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2832\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3876\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.275\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.73s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130932\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130934\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130934\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.72 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.511765188075606, 5.693462742249347, 6.80005, 0.43879)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8001.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2672\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.28\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5251\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.26\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.96s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130934\\\")\n"," 17%|█▋        | 17/100 [02:52<14:56, 10.80s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130936\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130936\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.71 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.835144197522325, 6.679599185844383, 7.23528, 0.36852)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7996.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2448\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2558\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2729\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2409\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.02s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130936\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130938\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130938\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.71 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.835144197522325, 6.679599185844383, 7.24052, 0.3681)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7988.7 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2401\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2458\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2585\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2334\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.8s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130938\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130940\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130940\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.70 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.835144197522325, 6.679599185844383, 7.23938, 0.3674)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7982.65 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3141\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5273\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5427\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3141\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130940\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130941\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130941\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.70 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.835144197522325, 6.687856457832816, 7.24197, 0.36692)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2833\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2494\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2523\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2406\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.63s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130941\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130943\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130943\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.70 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.829550831490737, 6.679599185844383, 7.22461, 0.36794)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7992.72 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2871\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2793\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3034\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2716\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.68s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130943\\\")\n"," 18%|█▊        | 18/100 [03:02<14:04, 10.30s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130945\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130945\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.70 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.9395580252504825, 6.372090365202501, 7.16284, 0.49097)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7992.13 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1959\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2581\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2573\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1915\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.87s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130945\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130947\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130947\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.69 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.9433993684670785, 6.372090365202501, 7.17015, 0.48994)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7987.99 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.259\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2811\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.245\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.234\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.68s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130947\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130949\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130949\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.69 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.9433993684670785, 6.372090365202501, 7.16672, 0.48768)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7987.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2617\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2043\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2287\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2013\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.81s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130949\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130950\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130950\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.69 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.9433993684670785, 6.385531610263777, 7.17352, 0.4871)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7981.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3451\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2323\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2281\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2203\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.76s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130950\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130952\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130952\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.68 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.9433993684670785, 6.372090365202501, 7.14927, 0.49143)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7979.82 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1933\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2786\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2151\t = Validation score   (-SMAPE)\n","\t1.11s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.188\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.25s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130952\\\")\n"," 19%|█▉        | 19/100 [03:11<13:43, 10.17s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130955\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130955\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.67 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.230737033890263, 6.509603364168197, 7.32567, 0.53359)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7971.97 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2425\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.6413\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.7483\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2425\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130955\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130956\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130956\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.67 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.230737033890263, 6.509603364168197, 7.33148, 0.53488)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7974.53 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2829\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2838\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.644\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2712\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.67s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130956\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_130958\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_130958\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.67 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.216315099892608, 6.5266119830190386, 7.3298, 0.53361)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7975.1 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4736\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.221\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.806\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.221\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.51s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_130958\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131000\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131000\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.67 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.230737033890263, 6.509603364168197, 7.33144, 0.53585)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7971.58 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-2.1822\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.6368\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.9122\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.6368\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.43s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131000\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131001\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131001\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.67 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.230737033890263, 6.509603364168197, 7.31672, 0.53017)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7984.76 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.531\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3106\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9413\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3106\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.48s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131001\\\")\n"," 20%|██        | 20/100 [03:19<12:40,  9.51s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131003\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131003\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.67 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.220438083175415, 6.946745196273333, 7.54423, 0.36652)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7975.5 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3889\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8621\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6033\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3889\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.53s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131003\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131004\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131004\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.67 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.220438083175415, 6.946745196273333, 7.54395, 0.36609)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7987.12 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2462\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3204\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5988\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2462\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131004\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131006\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131006\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.66 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.213612062811078, 6.946745196273333, 7.54762, 0.36442)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7969.92 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2732\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2521\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9243\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2506\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.58s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131006\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131008\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131008\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.66 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.220438083175415, 6.946745196273333, 7.54698, 0.36534)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7984.76 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3024\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2796\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2841\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2708\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.06s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131008\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131010\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131010\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.66 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.220438083175415, 6.951063779582331, 7.53703, 0.36491)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7970.48 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2712\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.0692\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.3458\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2712\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131010\\\")\n"," 21%|██        | 21/100 [03:28<12:12,  9.28s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131011\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131011\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.65 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.654898099303095, 6.752270376141742, 7.09688, 0.24721)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7986.92 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2275\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2118\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.4716\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2108\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.6s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131011\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131013\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131013\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.65 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.654898099303095, 6.752270376141742, 7.09996, 0.24666)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7979.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2195\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2221\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2313\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.213\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.67s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131013\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131015\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131015\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.65 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.654898099303095, 6.752270376141742, 7.09821, 0.24518)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7992.71 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1938\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1907\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1916\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1834\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.14s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131015\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131017\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131017\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.64 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.568751099524998, 6.75437097208278, 7.09986, 0.24418)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7979.76 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2058\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2151\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2216\t = Validation score   (-SMAPE)\n","\t0.9s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2042\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.19s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131017\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131019\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131019\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.64 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.654898099303095, 6.752270376141742, 7.09136, 0.24591)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7977.23 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2378\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.232\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2126\t = Validation score   (-SMAPE)\n","\t0.93s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.212\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131019\\\")\n"," 22%|██▏       | 22/100 [03:38<12:18,  9.47s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131021\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131021\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.63 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.522075678996111, 6.8679744089702925, 7.13413, 0.20158)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7988.05 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.1948\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.1959\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.2189\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.1948\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.44s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131021\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131023\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131023\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.63 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.506371975695849, 6.8679744089702925, 7.13555, 0.20065)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7987.01 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.233\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.2355\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.2578\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.233\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.36s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131023\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131024\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131024\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.63 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.522075678996111, 6.8679744089702925, 7.13503, 0.20041)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7994.21 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.2408\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.2374\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.2628\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.2374\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131024\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131026\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131026\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.63 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.522075678996111, 6.8679744089702925, 7.13643, 0.2013)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7984.0 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.2183\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1012\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0832\t = Validation score   (-SMAPE)\n","\t0.8s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0827\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.64s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131026\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131027\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131027\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.63 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.522075678996111, 6.8679744089702925, 7.12974, 0.20031)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.2 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.2161\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.2159\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.239\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.2159\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.36s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131027\\\")\n"," 23%|██▎       | 23/100 [03:46<11:23,  8.87s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131029\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131029\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.63 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.729157974631718, 8.726221591038756, 9.19289, 0.27209)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7993.58 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0926\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1115\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1168\t = Validation score   (-SMAPE)\n","\t0.95s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0912\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.03s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131029\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131031\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131031\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.62 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.729157974631718, 8.726221591038756, 9.19505, 0.27274)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7986.74 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1308\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1158\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1499\t = Validation score   (-SMAPE)\n","\t0.8s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1152\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.74s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131031\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131033\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131033\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.62 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.729157974631718, 8.726221591038756, 9.19219, 0.26886)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7976.58 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0959\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0913\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.127\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.088\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.67s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131033\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131034\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131034\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.62 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.727442258733658, 8.731852856122137, 9.19777, 0.26938)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.05 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0963\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1034\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1132\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0942\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131034\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131036\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131036\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.62 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.729157974631718, 8.726221591038756, 9.18693, 0.2708)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7970.66 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0885\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1125\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1073\t = Validation score   (-SMAPE)\n","\t0.89s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0875\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.89s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131036\\\")\n"," 24%|██▍       | 24/100 [03:55<11:31,  9.10s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131038\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131038\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.61 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.442721128642875, 8.420444377273604, 8.85596, 0.31057)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7962.6 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.123\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1227\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.152\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1179\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131038\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131040\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131040\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.61 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.44383338240207, 8.394627738209456, 8.85897, 0.31151)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7982.52 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.119\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1161\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1924\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1127\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.74s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131040\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131042\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131042\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.61 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.44383338240207, 8.394627738209456, 8.85359, 0.30957)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7982.63 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.089\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0953\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1441\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0883\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.83s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131042\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131044\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131044\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.60 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.44383338240207, 8.394627738209456, 8.86125, 0.30946)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8003.85 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1602\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0933\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1127\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0932\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131044\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131046\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131046\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.60 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.44383338240207, 8.394627738209456, 8.84793, 0.31)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7983.67 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.163\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1323\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1466\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1322\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.57s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131046\\\")\n"," 25%|██▌       | 25/100 [04:04<11:17,  9.03s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131047\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131047\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.60 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.715473395565757, 7.63729224632495, 8.15001, 0.25611)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.95 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.102\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1358\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1397\t = Validation score   (-SMAPE)\n","\t0.89s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1016\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.04s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131047\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131049\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131049\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.59 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.723940658572019, 7.6594920709522425, 8.15121, 0.25749)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7991.46 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1379\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1582\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.133\t = Validation score   (-SMAPE)\n","\t0.91s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1269\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.85s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131049\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131051\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131051\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.59 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.723940658572019, 7.63729224632495, 8.14716, 0.25454)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7990.34 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.133\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1127\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1405\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1109\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131051\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131053\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131053\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.58 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.723940658572019, 7.63729224632495, 8.15292, 0.2551)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7995.1 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1046\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1366\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1388\t = Validation score   (-SMAPE)\n","\t0.82s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1042\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.85s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131053\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131055\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131055\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.58 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.723940658572019, 7.63729224632495, 8.14458, 0.25559)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7987.15 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1459\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1473\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.131\t = Validation score   (-SMAPE)\n","\t0.81s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1279\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.79s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131055\\\")\n"," 26%|██▌       | 26/100 [04:14<11:25,  9.27s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131057\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131057\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.58 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (10.146017958450518, 9.406022280471975, 9.76596, 0.18263)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7970.96 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1202\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0858\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0859\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.083\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.66s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131057\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131059\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131059\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.57 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (10.143157782847288, 9.406022280471975, 9.76648, 0.18398)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7981.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0647\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0873\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0751\t = Validation score   (-SMAPE)\n","\t0.82s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0628\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131059\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131101\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131101\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.57 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (10.146017958450518, 9.406022280471975, 9.76563, 0.18206)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7970.97 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0873\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0805\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0824\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0765\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131101\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131102\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131102\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.57 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (10.146017958450518, 9.424499685618889, 9.76855, 0.18161)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7986.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0995\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0843\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0791\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0774\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131102\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131104\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131104\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.57 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (10.146017958450518, 9.406022280471975, 9.76318, 0.18214)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7978.97 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1217\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1027\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1055\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1009\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.52s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131104\\\")\n"," 27%|██▋       | 27/100 [04:23<11:04,  9.10s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131106\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131106\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.56 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.2106245519569, 6.74467148409559, 7.40562, 0.33483)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7986.91 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2289\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5034\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2199\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2113\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.85s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131106\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131108\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131108\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.56 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.2106245519569, 6.928420279425721, 7.40555, 0.33621)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.24 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1717\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00139827\tvalid_set's SMAPE: -1.3484\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2264\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9487\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1717\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.97s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131108\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131110\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131110\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.56 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.153142542286595, 6.74467148409559, 7.39966, 0.33581)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7970.56 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2077\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2179\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.244\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2047\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.86s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131110\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131112\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131112\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.55 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.2106245519569, 6.74467148409559, 7.40786, 0.33437)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7981.48 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.199\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2105\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2435\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1957\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.95s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131112\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131114\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131114\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.55 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.2106245519569, 6.74467148409559, 7.39657, 0.33405)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.29 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.205\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2557\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2558\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2045\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.85s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131114\\\")\n"," 28%|██▊       | 28/100 [04:32<11:12,  9.34s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131116\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131116\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.54 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.889871203876463, 6.751569195792809, 7.26079, 0.25322)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7991.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2251\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1723\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2024\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1723\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.88s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131116\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131118\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131118\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.54 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.889871203876463, 6.763422908092217, 7.26297, 0.25551)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8003.08 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1694\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2095\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.213\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1694\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.93s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131118\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131120\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131120\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.54 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.889871203876463, 6.751569195792809, 7.25922, 0.25312)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7988.79 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1681\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1826\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2015\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1646\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.77s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131120\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131122\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131122\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.53 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.889871203876463, 6.751569195792809, 7.26507, 0.25369)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8000.35 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2413\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2005\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3325\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2005\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131122\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131123\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131123\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.53 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.875613267712699, 6.751569195792809, 7.25633, 0.25205)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7990.19 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1773\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1731\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1926\t = Validation score   (-SMAPE)\n","\t0.8s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1671\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.86s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131123\\\")\n"," 29%|██▉       | 29/100 [04:42<11:05,  9.38s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131125\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131125\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.53 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.384299968771254, 5.268888555857225, 6.70941, 0.28944)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7982.95 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3919\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3919\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6067\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3815\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.83s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131125\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131127\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131127\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.52 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.411495863024897, 5.268888555857225, 6.71032, 0.30114)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7984.29 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.353\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3956\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4447\t = Validation score   (-SMAPE)\n","\t0.96s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.353\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.33s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131127\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131130\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131130\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.52 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.411495863024897, 5.268888555857225, 6.70778, 0.29761)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7980.85 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.415\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5051\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.411\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3977\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.79s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131130\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131131\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131131\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.51 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.411495863024897, 5.268888555857225, 6.71233, 0.29785)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7982.09 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3579\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3369\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3925\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3328\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131131\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131133\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131133\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.51 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.411495863024897, 5.300814246746624, 6.70719, 0.29107)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7975.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4213\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4737\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5666\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4166\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.64s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131133\\\")\n"," 30%|███       | 30/100 [04:52<11:07,  9.53s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131135\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131135\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.51 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.844899066651388, 6.45356200478262, 7.18242, 0.28241)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7972.32 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2267\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2056\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.262\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2036\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.68s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131135\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131137\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131137\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.50 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.844899066651388, 6.45356200478262, 7.18518, 0.28545)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7981.0 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2299\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1891\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2195\t = Validation score   (-SMAPE)\n","\t0.9s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1886\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.97s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131137\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131139\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131139\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.50 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.844899066651388, 6.62602877023563, 7.18039, 0.2819)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7979.14 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.21\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2344\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2504\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2098\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.76s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131139\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131141\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131141\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.844899066651388, 6.45356200478262, 7.18866, 0.28115)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7972.25 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2403\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2403\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2511\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2329\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.61s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131141\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131142\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131142\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.805050737562144, 6.45356200478262, 7.17656, 0.2822)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8028.62 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2551\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2354\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2467\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2312\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131142\\\")\n"," 31%|███       | 31/100 [05:01<10:47,  9.38s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131144\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131144\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.237297735154995, 9.125910387596637, 9.20064, 0.01995)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8024.03 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0828\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0823\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0837\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0823\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.48s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131144\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131146\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131146\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.236736897934977, 9.127241331806468, 9.20071, 0.02011)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8030.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0801\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0799\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0813\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0799\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131146\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131147\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131147\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.237297735154995, 9.125910387596637, 9.20048, 0.02056)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8017.97 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0741\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.074\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0754\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.074\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131147\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131149\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131149\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.237297735154995, 9.125910387596637, 9.20095, 0.01984)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8008.64 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0847\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0842\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0857\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0842\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.34s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131149\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131150\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131150\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.237297735154995, 9.125910387596637, 9.20059, 0.02036)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8004.23 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.078\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0779\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0791\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0779\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.42s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131150\\\")\n"," 32%|███▏      | 32/100 [05:08<09:56,  8.77s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131151\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131151\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.11670199393221, 9.018758470234028, 9.07442, 0.01937)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    8000.88 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0846\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.084\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0855\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.084\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131151\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131153\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131153\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.115752731077743, 9.018758470234028, 9.07444, 0.01951)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7998.59 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0808\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0804\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0818\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0804\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131153\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131154\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131154\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.11670199393221, 9.018758470234028, 9.07435, 0.01951)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7999.63 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0807\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0803\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0818\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0803\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131154\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131156\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131156\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.11670199393221, 9.0214581409465, 9.0747, 0.01928)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7989.65 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0846\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.084\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0855\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.084\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.5s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131156\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131157\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131157\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.49 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (9.11670199393221, 9.018758470234028, 9.07441, 0.01955)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7968.13 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.1s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.07s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0789\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0783\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0797\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0783\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131157\\\")\n"," 33%|███▎      | 33/100 [05:16<09:20,  8.37s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131159\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131159\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.191163837591892, 8.026261694028232, 8.1225, 0.0354)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7968.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1787\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1774\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1805\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1774\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.49s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131159\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131200\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131200\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.188466877526963, 7.875590473698406, 8.12257, 0.03612)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7951.87 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1707\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1704\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1733\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1704\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.44s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131200\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131202\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131202\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.191163837591892, 7.875590473698406, 8.12246, 0.03644)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7956.69 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1637\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1631\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1662\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1631\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.43s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131202\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131203\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131203\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.191163837591892, 7.875590473698406, 8.12282, 0.03567)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7942.9 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1804\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1792\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1824\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1792\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131203\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131205\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131205\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.191163837591892, 7.875590473698406, 8.12213, 0.03627)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7956.35 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1686\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1675\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1705\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1675\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.44s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131205\\\")\n"," 34%|███▍      | 34/100 [05:23<08:54,  8.09s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131206\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131206\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.728943809422966, 7.632013454567135, 7.68719, 0.01933)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7937.95 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1013\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1009\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1023\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1009\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131206\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131208\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131208\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.728151660457546, 7.630703950778872, 7.687, 0.01956)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7898.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0961\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0957\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0973\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0957\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.66s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131208\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131210\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131210\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.728943809422966, 7.630703950778872, 7.68688, 0.01953)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7898.54 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0972\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0969\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0985\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0969\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131210\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131211\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131211\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.728943809422966, 7.630703950778872, 7.68728, 0.01932)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7890.51 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.1s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.07s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1004\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0999\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1017\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0999\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.42s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131211\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131213\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131213\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.728943809422966, 7.630703950778872, 7.68684, 0.01964)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7896.9 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0946\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0942\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0957\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0942\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.44s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131213\\\")\n"," 35%|███▌      | 35/100 [05:31<08:40,  8.00s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131214\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131214\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.037026088604039, 7.925865865766093, 7.97121, 0.01926)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7914.01 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0961\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0954\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0967\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0954\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.46s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131214\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131216\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131216\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.038731493340315, 7.925865865766093, 7.97132, 0.01949)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7918.21 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0916\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0907\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0921\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0907\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131216\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131217\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131217\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.038731493340315, 7.925865865766093, 7.97126, 0.01957)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7899.81 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0906\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0896\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.091\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0896\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.37s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131217\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131219\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131219\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.038731493340315, 7.925865865766093, 7.9712, 0.01894)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7914.19 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1024\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1016\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1029\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1016\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131219\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131220\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131220\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.038731493340315, 7.928204204014009, 7.9709, 0.01938)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7909.14 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0948\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.0941\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.0953\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0941\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131220\\\")\n"," 36%|███▌      | 36/100 [05:38<08:18,  7.79s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131221\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131221\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.48 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.77040803498577, 6.771706434518861, 7.89728, 0.7266)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7933.39 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2303\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00345901\tvalid_set's SMAPE: -1.55293\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3233\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2844\t = Validation score   (-SMAPE)\n","\t0.94s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2299\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.16s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131221\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131224\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131224\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.47 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.77040803498577, 6.667465568832957, 7.90342, 0.72352)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7895.39 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.23\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2604\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2789\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2285\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.24s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131224\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131226\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131226\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.46 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.77040803498577, 6.667465568832957, 7.89789, 0.72264)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7879.13 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2587\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2903\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2744\t = Validation score   (-SMAPE)\n","\t0.94s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2507\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.06s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131226\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131228\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131228\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.46 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.77040803498577, 6.667465568832957, 7.91192, 0.72191)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7900.96 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2671\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2932\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2818\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2607\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.92s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131228\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131230\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131230\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.45 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.730593430447456, 6.667465568832957, 7.87105, 0.72867)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7911.4 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2514\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2619\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.249\t = Validation score   (-SMAPE)\n","\t1.1s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2339\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131230\\\")\n"," 37%|███▋      | 37/100 [05:50<09:19,  8.88s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131233\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131233\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.44 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.184558382442276, 6.078192550617806, 7.22276, 0.79473)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7903.81 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.373\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3431\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7009\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3348\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.88s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131233\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131235\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131235\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.44 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.186865239955413, 6.0690735264349644, 7.23264, 0.79471)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7892.87 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2857\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3003\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2822\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2579\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.98s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131235\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131237\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131237\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.44 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.186865239955413, 6.0690735264349644, 7.2286, 0.78991)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7895.02 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3451\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.353\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4319\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3312\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.01s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131237\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131239\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131239\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.43 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.186865239955413, 6.0690735264349644, 7.24426, 0.78987)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7897.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3317\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3073\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4149\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3031\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.05s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131239\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131241\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131241\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.43 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.186865239955413, 6.0690735264349644, 7.1949, 0.79125)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7889.72 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3493\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3396\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3324\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3195\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.79s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131241\\\")\n"," 38%|███▊      | 38/100 [06:00<09:35,  9.28s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131243\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131243\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.42 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.309735963896149, 6.180844468269383, 7.41566, 0.79032)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7890.33 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2771\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3269\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3593\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2753\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.93s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131243\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131245\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131245\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.42 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.309735963896149, 6.180844468269383, 7.42769, 0.78963)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7897.49 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2383\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00297893\tvalid_set's SMAPE: -2.08269\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.26\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3258\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2291\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.3s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131245\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131247\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131247\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.42 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.309735963896149, 6.180844468269383, 7.4186, 0.78784)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7897.35 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.273\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3069\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3511\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2729\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.15s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131247\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131250\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131250\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.42 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.309735963896149, 6.203425809019622, 7.43687, 0.78446)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7891.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2323\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00256768\tvalid_set's SMAPE: -2.06237\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2287\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2861\t = Validation score   (-SMAPE)\n","\t0.91s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2161\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131250\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131252\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131252\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.41 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.30796240715533, 6.180844468269383, 7.39102, 0.7905)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7894.15 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2762\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2637\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3331\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2493\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.65s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131252\\\")\n"," 39%|███▉      | 39/100 [06:11<09:57,  9.80s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131254\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131254\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.41 GB / 498.62 GB (25.2%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.385443068531238, 5.792098555704769, 7.19961, 0.87926)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7916.42 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6157\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5839\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5862\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5547\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.27s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131254\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131256\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131256\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.40 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.385443068531238, 5.7791991139409555, 7.20615, 0.88082)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7929.23 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5532\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0129541\tvalid_set's SMAPE: -1.81975\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.6451\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6658\t = Validation score   (-SMAPE)\n","\t0.86s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5526\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.28s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131256\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131259\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131259\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.39 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.385443068531238, 5.7791991139409555, 7.20078, 0.87782)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7931.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4986\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.012826\tvalid_set's SMAPE: -1.83351\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.6205\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6264\t = Validation score   (-SMAPE)\n","\t0.99s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4986\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131259\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131301\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131301\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.39 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.385443068531238, 5.7791991139409555, 7.21814, 0.87809)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7902.34 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6068\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5896\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5629\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5487\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.97s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131301\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131303\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131303\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.38 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.356907061482895, 5.7791991139409555, 7.16739, 0.87743)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7912.71 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5713\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5668\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6499\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5412\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131303\\\")\n"," 40%|████      | 40/100 [06:22<10:14, 10.25s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131305\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131305\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.38 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.730690365678642, 6.7259935654466965, 7.89608, 0.67854)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7930.28 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.263\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00508693\tvalid_set's SMAPE: -1.91743\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3122\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3328\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2629\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.29s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131305\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131308\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131308\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.37 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.730690365678642, 6.7259935654466965, 7.90171, 0.67923)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7923.72 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3153\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3155\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.346\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3064\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.13s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131308\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131310\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131310\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.37 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.730690365678642, 6.726712903686563, 7.89465, 0.67898)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7921.73 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2824\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00517306\tvalid_set's SMAPE: -1.92629\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3186\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3267\t = Validation score   (-SMAPE)\n","\t1.03s\t = Training   runtime\n","\t0.02s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.28\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131310\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131312\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131312\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.36 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.730690365678642, 6.7259935654466965, 7.91518, 0.67313)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7916.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2935\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3298\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.356\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2935\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.13s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131312\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131315\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131315\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.35 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.728166990783354, 6.7259935654466965, 7.87716, 0.6803)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7935.32 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2497\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00359901\tvalid_set's SMAPE: -1.91238\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2719\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3089\t = Validation score   (-SMAPE)\n","\t1.01s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.249\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131315\\\")\n"," 41%|████      | 41/100 [06:34<10:37, 10.80s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131317\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131317\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.35 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.15065259875299, 5.786038116598931, 6.97288, 0.95039)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7928.35 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2945\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3037\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3117\t = Validation score   (-SMAPE)\n","\t0.81s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2765\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.0s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131317\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131320\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131320\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.34 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.15065259875299, 5.786038116598931, 6.98625, 0.9494)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7923.25 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2948\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3288\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4135\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2874\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.98s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131320\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131322\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131322\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.34 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.148364691304934, 5.794841205603168, 6.97323, 0.94597)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7941.56 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3249\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3242\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3153\t = Validation score   (-SMAPE)\n","\t0.98s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2933\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.01s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131322\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131324\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131324\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.33 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.15065259875299, 5.786038116598931, 6.99908, 0.94838)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7929.01 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2904\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2553\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2939\t = Validation score   (-SMAPE)\n","\t0.84s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2516\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.97s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131324\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131326\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131326\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.33 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.15065259875299, 5.786038116598931, 6.94024, 0.94714)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7928.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3095\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3292\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3754\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2933\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.86s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131326\\\")\n"," 42%|████▏     | 42/100 [06:45<10:18, 10.67s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131328\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131328\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.32 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.470436957407792, 5.676753802268282, 7.12778, 0.98881)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7925.71 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3207\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3555\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3702\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.31\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.71s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131328\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131330\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131330\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.32 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.470436957407792, 5.5943395632748185, 7.14553, 0.98851)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7931.31 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3117\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3623\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3689\t = Validation score   (-SMAPE)\n","\t0.84s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.311\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131330\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131331\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131331\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.32 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.470436957407792, 5.5943395632748185, 7.12663, 0.98511)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7928.12 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8673\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5283\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3857\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3857\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131331\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131333\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131333\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.31 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.470436957407792, 5.5943395632748185, 7.1619, 0.98567)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7928.37 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4682\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2988\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8428\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2988\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.65s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131333\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131335\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131335\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.31 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.452419976073468, 5.5943395632748185, 7.10284, 0.98693)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7933.92 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3666\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.356\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2804\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2804\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.67s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131335\\\")\n"," 43%|████▎     | 43/100 [06:53<09:35, 10.09s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131337\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131337\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.31 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.544691620118048, 6.140315493840337, 7.29696, 0.92138)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7922.75 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1861\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1733\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3769\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1661\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.97s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131337\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131339\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131339\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.30 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.544691620118048, 6.149578790201674, 7.31222, 0.92078)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7931.28 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6657\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2469\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2584\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2426\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.7s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131339\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131340\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131340\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.30 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.544691620118048, 6.140315493840337, 7.29693, 0.91799)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7930.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4771\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1942\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.217\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1927\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.61s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131340\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131342\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131342\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.30 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.544691620118048, 6.140315493840337, 7.32798, 0.91788)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7932.6 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1826\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5937\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2929\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1825\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131342\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131344\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131344\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.30 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.540761269515617, 6.140315493840337, 7.27372, 0.9205)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7928.1 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.197\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.173\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1874\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1694\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.9s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131344\\\")\n"," 44%|████▍     | 44/100 [07:03<09:12,  9.86s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131346\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131346\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.29 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.321713444084502, 7.479706105135553, 7.98659, 0.19028)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7943.45 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.971\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.9661\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9859\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.9661\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131346\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131347\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131347\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.29 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.321713444084502, 7.4990247607193545, 7.9882, 0.19086)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7936.34 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.9691\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.9645\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9857\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.9645\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.44s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131347\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131349\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131349\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.29 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.321713444084502, 7.479706105135553, 7.98846, 0.18801)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7972.93 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.0204\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.0182\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.0374\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.0182\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.37s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131349\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131350\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131350\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.29 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.321713444084502, 7.479706105135553, 7.98893, 0.18921)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7943.16 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.9896\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.9859\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.0063\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.9859\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131350\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131352\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131352\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.29 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.320662359326757, 7.479706105135553, 7.98323, 0.1901)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7952.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.9592\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.9564\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9748\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.9564\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.36s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131352\\\")\n"," 45%|████▌     | 45/100 [07:10<08:18,  9.07s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131353\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131353\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.29 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.215212039328724, 7.061711654219195, 7.69933, 0.27538)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7948.38 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2064\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2065\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2594\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1982\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.9s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131353\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131355\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131355\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.29 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.215212039328724, 7.095296522810252, 7.70282, 0.27618)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7942.7 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2285\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3135\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3278\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2271\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.7s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131355\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131357\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131357\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.28 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.215212039328724, 7.061711654219195, 7.70114, 0.27279)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7958.39 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1908\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2105\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2166\t = Validation score   (-SMAPE)\n","\t1.25s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1907\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.59s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131357\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131400\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131400\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.27 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.215212039328724, 7.061711654219195, 7.70667, 0.272)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7933.37 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2311\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2125\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2187\t = Validation score   (-SMAPE)\n","\t0.91s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2068\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.93s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131400\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131402\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131402\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.27 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.209221333025772, 7.061711654219195, 7.69335, 0.2755)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7909.93 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1855\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00142677\tvalid_set's SMAPE: -1.67272\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2383\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2984\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1855\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.94s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131402\\\")\n"," 46%|████▌     | 46/100 [07:20<08:33,  9.51s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131404\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131404\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.27 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.963052682631927, 8.224967478914584, 8.58786, 0.18552)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7925.72 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.183\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1938\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2397\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1811\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.57s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131404\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131405\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131405\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.27 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.963052682631927, 8.223423292361721, 8.58936, 0.18537)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7931.24 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1422\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2957\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2636\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1422\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.55s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131405\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131407\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131407\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.27 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.963052682631927, 8.223423292361721, 8.58851, 0.18301)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7947.12 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.07s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1264\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1454\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1407\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1247\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.8s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131407\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131409\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131409\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.27 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.958802490159966, 8.223423292361721, 8.5906, 0.18421)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7939.95 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1223\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3021\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.291\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.122\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.52s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131409\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131410\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131410\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.26 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.963052682631927, 8.223423292361721, 8.58466, 0.18363)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7944.05 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1435\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1707\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1612\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1405\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.66s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131410\\\")\n"," 47%|████▋     | 47/100 [07:29<08:06,  9.18s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131412\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131412\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.26 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.993741526942755, 7.109814101459124, 7.53179, 0.23128)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7951.79 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1125\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1277\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1287\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.111\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.72s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131412\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131414\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131414\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.26 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.993741526942755, 7.109814101459124, 7.53284, 0.23088)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7945.96 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1153\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1275\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7063\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1138\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.64s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131414\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131416\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131416\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.26 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.9822524549413805, 7.109814101459124, 7.53222, 0.23001)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7937.45 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.0981\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1171\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3232\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.0979\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.76s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131416\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131417\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131417\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.25 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.993741526942755, 7.111870942811762, 7.53461, 0.22968)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7948.3 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.193\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3064\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4896\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.193\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.42s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131417\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131419\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131419\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.25 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.993741526942755, 7.109814101459124, 7.52668, 0.2303)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7946.64 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1037\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1162\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2322\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.102\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.89s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131419\\\")\n"," 48%|████▊     | 48/100 [07:38<07:51,  9.08s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131421\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131421\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.25 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.357061997243552, 7.597697328593134, 8.02651, 0.17148)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7951.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8187\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8145\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8309\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8145\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.44s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131421\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131422\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131422\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.25 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.36253971854307, 7.597697328593134, 8.02678, 0.17189)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7961.44 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8228\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8211\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8375\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8211\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131422\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131424\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131424\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.25 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.36253971854307, 7.597697328593134, 8.02655, 0.17008)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7955.39 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8473\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.844\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8607\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.844\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131424\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131425\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131425\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.25 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.36253971854307, 7.602920422085396, 8.02883, 0.16939)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7955.57 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8684\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8658\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8818\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8658\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.33s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131425\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131427\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131427\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.25 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.36253971854307, 7.597697328593134, 8.023, 0.17093)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7955.85 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8286\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8267\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8428\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8267\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131427\\\")\n"," 49%|████▉     | 49/100 [07:45<07:14,  8.52s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131428\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131428\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.25 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.334543580818517, 7.495097400645106, 8.02077, 0.17296)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7954.42 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.812\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8096\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8247\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8096\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131428\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131430\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131430\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.24 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.334543580818517, 7.495097400645106, 8.0224, 0.17354)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7945.52 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.832\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8304\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8458\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8304\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.46s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131430\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131431\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131431\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.24 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.332091774388623, 7.495097400645106, 8.02169, 0.17243)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7958.02 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8433\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8415\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8582\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8415\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.36s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131431\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131432\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131432\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.24 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.334543580818517, 7.495097400645106, 8.02375, 0.17173)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7953.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8361\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8352\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8511\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8352\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131432\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131434\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131434\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.24 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.334543580818517, 7.498426667952846, 8.01772, 0.17239)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7945.07 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8344\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8328\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8484\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8328\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.35s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131434\\\")\n"," 50%|█████     | 50/100 [07:52<06:46,  8.12s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131435\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131435\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.24 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.296177374868089, 7.240334232316201, 7.87325, 0.22082)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7957.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1939\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2022\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.16\t = Validation score   (-SMAPE)\n","\t0.93s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1588\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.8s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131435\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131437\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131437\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.24 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.296177374868089, 7.240334232316201, 7.87388, 0.22233)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7925.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2488\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2603\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2786\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2463\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131437\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131439\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131439\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.24 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.296177374868089, 7.252904051082247, 7.87358, 0.22093)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7940.17 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2521\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1597\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2433\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1593\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.5s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131439\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131440\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131440\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.24 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.287518357635841, 7.240334232316201, 7.87717, 0.21972)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7947.9 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1714\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2373\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2302\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1702\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131440\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131442\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131442\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.23 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.296177374868089, 7.240334232316201, 7.86765, 0.22155)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7960.24 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.196\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1507\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1716\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1504\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131442\\\")\n"," 51%|█████     | 51/100 [08:01<06:44,  8.25s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131444\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131444\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.23 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.111838077258517, 6.981470316804418, 7.63734, 0.28016)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7945.49 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.1s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.07s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3182\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3344\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2899\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2898\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131444\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131445\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131445\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.23 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.111838077258517, 6.964041089992729, 7.63837, 0.28014)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7958.59 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2762\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2119\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2822\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2119\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.63s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131445\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131447\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131447\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.23 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.111838077258517, 6.964041089992729, 7.63937, 0.27897)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7949.48 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2547\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2112\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3429\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2112\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.44s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131447\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131449\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131449\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.23 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.111838077258517, 6.964041089992729, 7.64215, 0.27672)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7964.34 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4009\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3037\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2643\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2643\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.5s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131449\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131450\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131450\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.22 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.108051332335632, 6.964041089992729, 7.63232, 0.27873)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7953.97 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2392\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2952\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.231\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.226\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.59s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131450\\\")\n"," 52%|█████▏    | 52/100 [08:09<06:33,  8.19s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131452\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131452\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.22 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.301194057862347, 6.4492057997815175, 7.46784, 0.58943)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7948.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4987\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.6483\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8063\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4987\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.71s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131452\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131454\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131454\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.22 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.29497920791996, 6.4492057997815175, 7.47473, 0.59084)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7951.26 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5035\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5173\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7567\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4916\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131454\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131455\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131455\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.22 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.301194057862347, 6.4597792509217555, 7.47725, 0.58883)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7955.91 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5362\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4853\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9835\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4851\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131455\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131457\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131457\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.22 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.301194057862347, 6.4492057997815175, 7.47068, 0.58902)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7963.28 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4302\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00788764\tvalid_set's SMAPE: -1.3907\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.5244\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4643\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.423\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.09s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131457\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131459\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131459\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.21 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.301194057862347, 6.4492057997815175, 7.45727, 0.58965)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7939.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4505\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4442\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.8164\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4344\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.68s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131459\\\")\n"," 53%|█████▎    | 53/100 [08:18<06:37,  8.46s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131501\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131501\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.21 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.234214706399841, 5.92607232834093, 7.28782, 0.73161)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7948.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6201\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.6368\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6441\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5955\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.72s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131501\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131503\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131503\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.20 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.234214706399841, 5.921256652064486, 7.29658, 0.73822)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7947.47 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.6485\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.6218\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6401\t = Validation score   (-SMAPE)\n","\t0.9s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.6065\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.03s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131503\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131505\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131505\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.20 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.234214706399841, 5.921256652064486, 7.28776, 0.73415)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7948.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5422\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.6008\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6158\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5367\t = Validation score   (-SMAPE)\n","\t0.11s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.0s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131505\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131507\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131507\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.19 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.234214706399841, 5.921256652064486, 7.31058, 0.72563)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7926.24 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5583\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5574\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5764\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5315\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.17s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131507\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131509\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131509\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.19 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.228080763159817, 5.921256652064486, 7.26243, 0.73585)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7930.73 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4831\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5166\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5104\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4669\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.18s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131509\\\")\n"," 54%|█████▍    | 54/100 [08:28<06:59,  9.12s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131512\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131512\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.107949497310722, 6.794676162530236, 6.9188, 0.08655)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7940.85 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5329\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5323\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5429\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5323\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.43s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131512\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131513\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131513\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.107949497310722, 6.79090680648553, 6.91924, 0.08702)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7954.99 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5218\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5209\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5314\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5209\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.35s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131513\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131515\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131515\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.100654032813861, 6.79090680648553, 6.9197, 0.08626)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7957.26 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5439\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5434\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5544\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5434\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131515\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131516\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131516\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.107949497310722, 6.79090680648553, 6.9195, 0.08621)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7953.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5443\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5432\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5541\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5432\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131516\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131517\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131517\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.107949497310722, 6.79090680648553, 6.91733, 0.08564)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7960.13 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5582\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5572\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5685\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5572\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.4s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131517\\\")\n"," 55%|█████▌    | 55/100 [08:36<06:25,  8.57s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131519\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131519\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.444708516387141, 7.859737426436143, 8.33818, 0.05197)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7964.52 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2467\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2462\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2511\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2462\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.4s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131519\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131520\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131520\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.444166480642531, 7.859737426436143, 8.33884, 0.05209)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7954.45 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2488\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.249\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2537\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2488\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.44s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131520\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131522\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131522\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.444708516387141, 8.24476485362715, 8.33888, 0.0504)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7962.91 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2605\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.26\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2646\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.26\t = Validation score   (-SMAPE)\n","\t0.11s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.53s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131522\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131523\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131523\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.444708516387141, 7.859737426436143, 8.33877, 0.05157)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7901.71 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2608\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.26\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2652\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.26\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.43s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131523\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131525\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131525\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.444708516387141, 7.859737426436143, 8.33701, 0.0517)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7909.23 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2563\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2552\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2602\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2552\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131525\\\")\n"," 56%|█████▌    | 56/100 [08:43<06:01,  8.22s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131526\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131526\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.18 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.156659383056487, 6.574126994492655, 7.44472, 0.5152)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7902.0 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3212\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4589\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3384\t = Validation score   (-SMAPE)\n","\t0.8s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3159\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.65s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131526\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131528\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131528\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.17 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.156659383056487, 6.437559631302042, 7.45121, 0.51868)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7905.66 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3411\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.331\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.6418\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3287\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.61s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131528\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131530\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131530\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.17 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.156659383056487, 6.437559631302042, 7.44561, 0.5124)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7912.19 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3623\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2955\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.511\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2955\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.61s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131530\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131531\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131531\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.17 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.156039662140582, 6.437559631302042, 7.46148, 0.51019)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7906.48 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2784\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2729\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3105\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2677\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.8s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131531\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131533\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131533\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.17 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.156659383056487, 6.437559631302042, 7.43187, 0.51947)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7910.94 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.3108\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3742\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9906\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3742\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.57s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131533\\\")\n"," 57%|█████▋    | 57/100 [08:52<05:58,  8.33s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131535\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131535\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.16 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.04821303824014, 7.915143694179116, 7.96915, 0.02634)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7877.24 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.13\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.13\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1321\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.13\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.57s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131535\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131537\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131537\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.16 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.04821303824014, 7.909695127056553, 7.96929, 0.02663)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7893.64 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1243\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1244\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1264\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1243\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131537\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131538\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131538\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.16 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.04821303824014, 7.909695127056553, 7.96903, 0.02615)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7898.87 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1332\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1328\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1352\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1328\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.49s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131538\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131540\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131540\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.16 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.04821303824014, 7.909695127056553, 7.96933, 0.02651)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7869.51 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1264\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1259\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1282\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1259\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.52s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131540\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131541\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131541\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.16 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.038963823353235, 7.909695127056553, 7.96864, 0.02627)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7874.37 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1313\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1309\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1334\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1309\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.45s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131541\\\")\n"," 58%|█████▊    | 58/100 [08:59<05:42,  8.15s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131543\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131543\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.16 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.213457614914581, 6.355968762161047, 7.2311, 0.6436)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7863.13 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2865\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3026\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2889\t = Validation score   (-SMAPE)\n","\t1.29s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2722\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.63s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131543\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131545\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131545\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.15 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.212286286298939, 6.355968762161047, 7.24121, 0.64272)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7867.9 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3561\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4163\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3102\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3075\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.88s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131545\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131547\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131547\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.15 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.213457614914581, 6.355968762161047, 7.23658, 0.64052)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7766.84 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2392\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00308472\tvalid_set's SMAPE: -1.73874\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2755\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2805\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2375\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.47s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131547\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131550\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131550\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.14 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.213457614914581, 6.35971226739933, 7.24491, 0.6382)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7782.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2978\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3261\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00637228\tvalid_set's SMAPE: -1.84244\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.4264\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2935\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.36s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131550\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131552\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131552\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.14 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.213457614914581, 6.355968762161047, 7.21054, 0.64291)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7845.01 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3416\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3779\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3445\t = Validation score   (-SMAPE)\n","\t0.81s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3215\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.77s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131552\\\")\n"," 59%|█████▉    | 59/100 [09:11<06:17,  9.20s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131554\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131554\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.14 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.506035171600237, 7.279401590611004, 7.92698, 0.39782)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7847.93 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.241\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.252\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2416\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2336\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131554\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131556\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131556\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.13 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.496672066062136, 7.279401590611004, 7.93319, 0.39693)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7886.57 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2154\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2229\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2513\t = Validation score   (-SMAPE)\n","\t0.8s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2114\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.83s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131556\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131558\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131558\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.13 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.506035171600237, 7.279401590611004, 7.9322, 0.39602)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7898.51 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2344\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.238\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2507\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.226\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.64s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131558\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131600\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131600\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.13 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.506035171600237, 7.279401590611004, 7.93488, 0.39389)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7905.52 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2209\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2309\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00295453\tvalid_set's SMAPE: -2.09425\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2448\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2186\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.34s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131600\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131602\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131602\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.12 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.506035171600237, 7.279401590611004, 7.91641, 0.39786)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7897.15 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2234\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.228\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2815\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2157\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.8s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131602\\\")\n"," 60%|██████    | 60/100 [09:21<06:17,  9.43s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131604\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131604\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.12 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.667948398860426, 6.73658728925827, 8.0111, 0.30772)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7880.79 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1313\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1754\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2121\t = Validation score   (-SMAPE)\n","\t0.84s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1308\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.22s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131604\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131607\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131607\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.11 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.706040078525332, 6.73658728925827, 8.01421, 0.31408)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7897.33 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1516\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2405\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2339\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1516\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131607\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131609\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131609\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.11 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.706040078525332, 7.283283380004987, 8.01396, 0.30839)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7892.33 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1593\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2012\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00545327\tvalid_set's SMAPE: -1.53122\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2302\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1593\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.28s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131609\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131611\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131611\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.10 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.706040078525332, 6.73658728925827, 8.02308, 0.3067)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7882.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1615\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1932\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.208\t = Validation score   (-SMAPE)\n","\t0.9s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1601\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.02s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131611\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131613\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131613\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.09 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.706040078525332, 6.73658728925827, 8.00889, 0.3153)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7888.93 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00119566\tvalid_set's SMAPE: -1.40081\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.1429\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1928\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2111\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.142\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.19s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131613\\\")\n"," 61%|██████    | 61/100 [09:32<06:27,  9.94s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131615\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131615\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.09 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.707467249770112, 6.286072579630496, 7.0159, 0.317)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7892.55 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2127\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2206\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2299\t = Validation score   (-SMAPE)\n","\t0.91s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2011\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.08s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131615\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131618\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131618\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.08 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.722323314746422, 6.286072579630496, 7.0189, 0.32301)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7878.3 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1609\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.000823572\tvalid_set's SMAPE: -1.6502\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.243\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2485\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1609\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.08s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131618\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131620\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131620\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.08 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.722323314746422, 6.286072579630496, 7.01712, 0.32062)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7871.65 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1653\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2257\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.229\t = Validation score   (-SMAPE)\n","\t0.8s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1632\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.86s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131620\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131622\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131622\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.07 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.722323314746422, 6.293086506293449, 7.02812, 0.31622)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7874.9 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2302\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3392\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.249\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2233\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.86s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131622\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131624\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131624\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.07 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.722323314746422, 6.286072579630496, 7.01526, 0.3253)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7868.05 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1925\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2663\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2616\t = Validation score   (-SMAPE)\n","\t0.81s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1921\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.95s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131624\\\")\n"," 62%|██████▏   | 62/100 [09:42<06:22, 10.05s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131626\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131626\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.07 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.354042381610556, 5.820260956047874, 6.58995, 0.3312)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7873.71 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2389\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2378\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5777\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2207\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.7s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131626\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131627\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131627\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.06 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.354042381610556, 5.820260956047874, 6.59494, 0.33707)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7870.14 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1979\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2406\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4102\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.194\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.97s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131627\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131630\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131630\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.06 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.348613270622064, 5.820260956047874, 6.5928, 0.336)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7866.04 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2068\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2936\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.344\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2068\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.77s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131630\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131631\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131631\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.06 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.354042381610556, 5.831677177618607, 6.60387, 0.33091)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7857.49 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2415\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2572\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2773\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2347\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.01s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131631\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131634\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131634\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.05 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.354042381610556, 5.820260956047874, 6.59108, 0.33933)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7868.86 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2062\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2643\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3904\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2062\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.02s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131634\\\")\n"," 63%|██████▎   | 63/100 [09:52<06:10, 10.01s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131636\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131636\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.05 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.746110414995936, 6.496563806500526, 7.05488, 0.27038)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7858.05 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1477\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1735\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1904\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1439\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.94s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131636\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131638\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131638\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.04 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.746110414995936, 6.496563806500526, 7.05776, 0.27533)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7856.27 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1338\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2065\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2162\t = Validation score   (-SMAPE)\n","\t0.91s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1338\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.34s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131638\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131640\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131640\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.04 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.746110414995936, 6.496563806500526, 7.05607, 0.27349)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7802.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1534\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1873\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.21\t = Validation score   (-SMAPE)\n","\t0.86s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.153\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.06s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131640\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131642\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131642\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.03 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.746110414995936, 6.502520144428027, 7.06534, 0.27107)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7793.11 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1576\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2319\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2006\t = Validation score   (-SMAPE)\n","\t1.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1552\t = Validation score   (-SMAPE)\n","\t0.11s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.36s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131642\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131645\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131645\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.03 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.734208826850307, 6.496563806500526, 7.05499, 0.27802)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7730.73 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1423\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2183\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2196\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1423\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.89s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131645\\\")\n"," 64%|██████▍   | 64/100 [10:03<06:12, 10.33s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131647\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131647\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.03 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.29496136781802, 5.3366723204245, 6.30658, 0.40331)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7732.31 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4711\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00773595\tvalid_set's SMAPE: -2.02638\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.5034\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5443\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4653\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.25s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131647\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131649\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131649\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.02 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.259383509659217, 5.3366723204245, 6.30615, 0.41029)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7689.17 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5568\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5644\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5728\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.532\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.8s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131649\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131651\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131651\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.02 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.29496136781802, 5.345296367742031, 6.30565, 0.40834)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7689.54 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5473\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5421\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5848\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5231\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.7s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131651\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131653\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131653\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.01 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.29496136781802, 5.3366723204245, 6.31576, 0.40356)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7765.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4842\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5092\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5318\t = Validation score   (-SMAPE)\n","\t1.0s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4752\t = Validation score   (-SMAPE)\n","\t0.11s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.02s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131653\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131655\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131655\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.01 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.29496136781802, 5.3366723204245, 6.3033, 0.41275)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7748.94 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5438\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.517\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5198\t = Validation score   (-SMAPE)\n","\t1.0s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.504\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.17s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131655\\\")\n"," 65%|██████▌   | 65/100 [10:14<06:03, 10.38s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131657\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131657\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.00 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (6.466377978524725, 5.58097433428329, 5.98391, 0.18325)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7739.38 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.2937\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.288\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.3108\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.288\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.42s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131657\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131659\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131659\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.00 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (6.466377978524725, 5.58097433428329, 5.98687, 0.18607)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7738.6 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.1829\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.1766\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.1958\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.1766\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.43s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131659\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131700\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131700\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.00 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (6.46174938681035, 5.58097433428329, 5.98678, 0.18568)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7746.7 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.1854\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.1828\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.2035\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.1828\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.39s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131700\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131702\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131702\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.00 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (6.466377978524725, 5.583345921534254, 5.9902, 0.18435)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7751.87 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.2451\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.2439\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.2636\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.2439\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.51s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131702\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131703\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131703\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.00 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (6.466377978524725, 5.58097433428329, 5.98554, 0.18787)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7781.56 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.1742\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.1685\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.1874\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.1685\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131703\\\")\n"," 66%|██████▌   | 66/100 [10:21<05:23,  9.52s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131705\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131705\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   125.00 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.862235423170859, 6.5248536892827245, 7.13499, 0.26491)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7813.89 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1788\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2155\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2133\t = Validation score   (-SMAPE)\n","\t0.93s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1773\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.94s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131705\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131707\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131707\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.99 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.862235423170859, 6.5248536892827245, 7.13972, 0.27108)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7811.42 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.167\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2165\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2293\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1668\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.05s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131707\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131709\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131709\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.99 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.836875361668217, 6.5248536892827245, 7.13739, 0.26873)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7802.53 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1889\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2227\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2193\t = Validation score   (-SMAPE)\n","\t0.82s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1856\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131709\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131711\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131711\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.98 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.862235423170859, 6.533963241328839, 7.14757, 0.2683)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7807.69 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1782\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2255\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2291\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1782\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.68s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131711\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131713\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131713\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.98 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.862235423170859, 6.5248536892827245, 7.13501, 0.27372)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7820.02 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2053\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2085\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2291\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1951\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.92s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131713\\\")\n"," 67%|██████▋   | 67/100 [10:31<05:18,  9.65s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131715\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131715\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.98 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.270448306849701, 6.923530198097499, 7.58749, 0.28483)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7773.82 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2206\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1758\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5121\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1748\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131715\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131717\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131717\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.97 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.281369586944653, 6.923530198097499, 7.58952, 0.28996)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7782.54 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1627\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2218\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2384\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1627\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.79s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131717\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131718\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131718\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.97 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.281369586944653, 6.923530198097499, 7.58882, 0.2882)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7796.56 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.152\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1793\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2118\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1507\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.67s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131718\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131720\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131720\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.97 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.281369586944653, 6.929712446334179, 7.59834, 0.28503)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7798.32 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1514\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1902\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5251\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1514\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.6s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131720\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131722\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131722\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.97 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.281369586944653, 6.923530198097499, 7.58693, 0.29347)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7802.86 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1438\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1752\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4166\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1429\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.9s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131722\\\")\n"," 68%|██████▊   | 68/100 [10:41<05:04,  9.52s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131724\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131724\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.96 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.845013824900802, 7.818269358078838, 8.39652, 0.2344)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7852.59 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2327\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1112\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1166\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1074\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131724\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131726\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131726\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.96 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.846353043314327, 7.818269358078838, 8.39755, 0.23443)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7830.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1203\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1212\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1204\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1118\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.12s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131726\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131728\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131728\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.95 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.846353043314327, 8.061455320632424, 8.39674, 0.23178)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7830.46 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.573\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1056\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3368\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1056\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.48s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131728\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131729\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131729\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.95 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.846353043314327, 7.818269358078838, 8.39899, 0.23315)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7827.2 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1135\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1095\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1205\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1024\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.68s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131729\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131731\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131731\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.95 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.846353043314327, 7.818269358078838, 8.39081, 0.23213)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7834.65 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1186\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1089\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.111\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1008\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.79s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131731\\\")\n"," 69%|██████▉   | 69/100 [10:50<04:52,  9.43s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131733\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131733\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.94 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.870372231798685, 7.0055532707054375, 8.31651, 0.35502)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7838.28 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1693\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2079\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1955\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1606\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.81s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131733\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131735\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131735\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.94 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.871558670593013, 7.0055532707054375, 8.32106, 0.35002)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7846.95 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2165\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1753\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2814\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1724\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131735\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131737\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131737\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.94 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.871558670593013, 7.0055532707054375, 8.31798, 0.35234)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7826.49 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1982\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2169\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.167\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1572\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.57s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131737\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131739\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131739\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.93 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.871558670593013, 7.008640796755996, 8.32113, 0.34938)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7830.84 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2299\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1609\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2355\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1566\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.76s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131739\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131740\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131740\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.93 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.871558670593013, 7.0055532707054375, 8.30891, 0.35248)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7835.33 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1447\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.157\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1353\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1261\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.0s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131740\\\")\n"," 70%|███████   | 70/100 [10:59<04:42,  9.43s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131743\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131743\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.93 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.310267418124862, 6.892154212753486, 7.67215, 0.20459)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7839.78 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1823\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00201722\tvalid_set's SMAPE: -2.03351\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2917\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2645\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1823\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.96s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131743\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131745\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131745\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.92 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.339414898352084, 6.892154212753486, 7.67131, 0.20516)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7833.67 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2518\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2299\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.24\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2249\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.68s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131745\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131746\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131746\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.92 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.339414898352084, 6.892154212753486, 7.67209, 0.20404)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7828.57 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1878\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2048\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2561\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1874\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.02s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131746\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131748\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131748\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.339414898352084, 6.892154212753486, 7.67062, 0.20533)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7837.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1822\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2396\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2281\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1822\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.67s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131748\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131750\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131750\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.339414898352084, 6.904028342574502, 7.67247, 0.20333)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7814.64 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2427\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2649\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2412\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2292\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.5s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131750\\\")\n"," 71%|███████   | 71/100 [11:09<04:31,  9.38s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131752\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131752\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.510780796076879, 6.845369106546041, 7.15752, 0.14392)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7825.71 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.806\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8014\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8161\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8014\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.37s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131752\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131753\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131753\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.510780796076879, 6.845369106546041, 7.15751, 0.14432)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7832.92 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7853\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7801\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7947\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7801\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.33s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131753\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131755\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131755\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.510780796076879, 6.845369106546041, 7.15734, 0.14385)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7836.17 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8101\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8053\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8204\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8053\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.35s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131755\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131756\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131756\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.5105838013403785, 6.845369106546041, 7.15865, 0.14416)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7831.72 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7881\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7846\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7985\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.7846\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.37s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131756\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131757\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131757\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.510780796076879, 6.855092626398831, 7.15562, 0.14234)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7835.1 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8334\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8276\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8418\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8276\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.35s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131757\\\")\n"," 72%|███████▏  | 72/100 [11:16<04:02,  8.67s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131759\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131759\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.493531447298333, 7.849963239152325, 8.2057, 0.12881)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7831.09 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5906\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5896\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5986\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5896\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.42s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131759\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131800\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131800\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.493531447298333, 7.853138685059564, 8.20642, 0.12964)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7824.7 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5832\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.58\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5911\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.58\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131800\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131802\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131802\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.493531447298333, 7.849963239152325, 8.2066, 0.12823)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7825.06 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5931\t = Validation score   (-SMAPE)\n","\t0.27s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5914\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6014\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5914\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.29s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131802\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131803\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131803\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.493531447298333, 7.849963239152325, 8.20838, 0.12853)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7831.81 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5879\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5874\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.597\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5874\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131803\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131804\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131804\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.485537392012226, 7.849963239152325, 8.20359, 0.12876)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7833.91 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5902\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5894\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5993\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5894\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.32s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131804\\\")\n"," 73%|███████▎  | 73/100 [11:23<03:41,  8.19s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131806\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131806\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.91 GB / 498.62 GB (25.1%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.881074904100922, 7.662543035690334, 8.25039, 0.38499)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7844.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2342\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2336\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.223\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2161\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.78s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131806\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131808\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131808\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.90 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.881074904100922, 7.645129849117091, 8.2523, 0.38688)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7841.95 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2084\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2293\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2603\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2062\t = Validation score   (-SMAPE)\n","\t0.11s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.74s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131808\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131810\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131810\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.90 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.881074904100922, 7.645129849117091, 8.2541, 0.3841)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7844.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2514\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2344\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2288\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2194\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.66s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131810\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131811\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131811\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.90 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.881074904100922, 7.645129849117091, 8.25151, 0.38456)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7831.3 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2245\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2117\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2162\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.01s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131811\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131813\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131813\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.89 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.877063823699338, 7.645129849117091, 8.24413, 0.38235)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7829.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5118\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2226\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2192\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2115\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131813\\\")\n"," 74%|███████▍  | 74/100 [11:32<03:41,  8.54s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131815\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131815\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.89 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.743252354624787, 5.784194344950063, 7.22874, 0.26207)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7835.98 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3107\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.292\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2815\t = Validation score   (-SMAPE)\n","\t1.05s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2697\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.21s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131815\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131818\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131818\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.88 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.743252354624787, 5.784194344950063, 7.23011, 0.26368)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7797.31 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2904\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3395\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2949\t = Validation score   (-SMAPE)\n","\t0.81s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2687\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.91s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131818\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131820\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131820\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.88 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.743252354624787, 5.784194344950063, 7.23097, 0.25101)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7788.04 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3512\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3041\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3637\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3015\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.0s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131820\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131822\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131822\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.87 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.726459642499187, 5.92895064171874, 7.23323, 0.25473)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7792.51 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3277\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3422\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3529\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3211\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.91s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131822\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131824\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131824\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.87 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.743252354624787, 5.784194344950063, 7.22645, 0.25361)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7796.61 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2824\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00787471\tvalid_set's SMAPE: -1.69636\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.309\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3159\t = Validation score   (-SMAPE)\n","\t0.89s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2744\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.14s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131824\\\")\n"," 75%|███████▌  | 75/100 [11:43<03:49,  9.19s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131826\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131826\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.86 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.493896145856914, 6.639535906776836, 7.0879, 0.1856)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7770.01 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.0243\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.0226\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.0407\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.0226\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.33s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131826\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131827\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131827\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.86 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.501479835217632, 6.597963920942255, 7.08954, 0.18709)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7781.25 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.9916\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.9892\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.0058\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.9892\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.36s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131827\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131829\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131829\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.86 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.501479835217632, 6.597963920942255, 7.08718, 0.18516)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7817.87 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.07s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.0398\t = Validation score   (-SMAPE)\n","\t0.27s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.035\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.0556\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.035\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.35s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131829\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131830\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131830\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.86 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.501479835217632, 6.597963920942255, 7.09113, 0.18578)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7824.11 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.0334\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.0319\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.0489\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.0319\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.34s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131830\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131832\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131832\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.86 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.501479835217632, 6.597963920942255, 7.08563, 0.18384)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7821.27 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-1.0995\t = Validation score   (-SMAPE)\n","\t0.28s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.0979\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.1161\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-1.0979\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.34s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131832\\\")\n"," 76%|███████▌  | 76/100 [11:50<03:24,  8.52s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131833\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131833\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.86 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.832553717468904, 6.75058671659146, 7.2645, 0.3314)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7816.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1922\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1645\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.177\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.16\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.91s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131833\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131835\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131835\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.85 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.830266622669348, 6.75058671659146, 7.26622, 0.33108)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7824.09 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2348\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1958\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.198\t = Validation score   (-SMAPE)\n","\t0.9s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1921\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131835\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131837\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131837\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.85 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.832553717468904, 6.75058671659146, 7.26574, 0.32723)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7839.59 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.174\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1969\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1981\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1735\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.49s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131837\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131838\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131838\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.85 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.832553717468904, 6.755209976513464, 7.2699, 0.32646)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7846.49 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2617\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1668\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3383\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1665\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131838\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131840\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131840\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.85 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.832553717468904, 6.75058671659146, 7.25738, 0.32913)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7850.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2701\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1948\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1667\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1667\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.57s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131840\\\")\n"," 77%|███████▋  | 77/100 [11:58<03:18,  8.62s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131842\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131842\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.84 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.193488699147414, 6.701518007734286, 7.31298, 0.47751)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7851.87 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1279\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1393\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3957\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1201\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.73s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131842\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131844\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131844\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.84 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.169585586061746, 6.702697195214312, 7.31739, 0.47698)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7856.89 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1421\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2012\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1798\t = Validation score   (-SMAPE)\n","\t0.94s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1421\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.13s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131844\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131846\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131846\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.84 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.193488699147414, 6.701518007734286, 7.31234, 0.47321)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7853.89 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1307\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1407\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2478\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1249\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.76s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131846\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131848\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131848\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.83 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.193488699147414, 6.701518007734286, 7.31865, 0.4742)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7852.49 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1344\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2503\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2848\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1341\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.65s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131848\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131849\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131849\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.83 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.193488699147414, 6.701518007734286, 7.30082, 0.47418)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7838.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.159\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1761\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.227\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1541\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.63s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131849\\\")\n"," 78%|███████▊  | 78/100 [12:08<03:14,  8.84s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131851\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131851\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.83 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.463800853472081, 7.33508624394092, 7.94657, 0.2825)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7847.57 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1595\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2021\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1869\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.157\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.51s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131851\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131853\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131853\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.82 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.47005965428218, 7.33508624394092, 7.94863, 0.2831)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7855.56 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1893\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2338\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3506\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1893\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.59s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131853\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131854\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131854\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.82 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.47005965428218, 7.33508624394092, 7.94646, 0.2797)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7842.75 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1744\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2031\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1342\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1342\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131854\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131856\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131856\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.82 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.47005965428218, 7.391982274864115, 7.95106, 0.27991)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7845.33 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1734\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1556\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1556\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1481\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.61s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131856\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131858\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131858\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.82 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.47005965428218, 7.33508624394092, 7.93924, 0.28132)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7842.18 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1398\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1816\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2929\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1397\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.51s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131858\\\")\n"," 79%|███████▉  | 79/100 [12:16<03:01,  8.63s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131859\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131859\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.82 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.823188547347971, 6.875190764108737, 7.67638, 0.61592)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7840.26 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1699\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2351\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.168\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.86s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131859\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131901\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131901\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.81 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.814348253089022, 6.866974950260517, 7.68055, 0.6154)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7815.91 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1617\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2352\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2108\t = Validation score   (-SMAPE)\n","\t0.8s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1616\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.94s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131901\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131903\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131903\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.81 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.823188547347971, 6.866974950260517, 7.67438, 0.61102)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7804.37 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1549\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1766\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1841\t = Validation score   (-SMAPE)\n","\t0.81s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1526\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.81s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131903\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131905\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131905\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.80 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.823188547347971, 6.866974950260517, 7.68318, 0.61159)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7825.7 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1788\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1787\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1908\t = Validation score   (-SMAPE)\n","\t0.82s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.168\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.95s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131905\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131907\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131907\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.80 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.823188547347971, 6.866974950260517, 7.65989, 0.612)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7818.8 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1408\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1593\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.184\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.136\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.17s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131907\\\")\n"," 80%|████████  | 80/100 [12:26<03:02,  9.10s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131909\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131909\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.79 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.903374617466499, 6.056502943287204, 7.08164, 0.67501)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7826.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4131\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2592\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2689\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.256\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131909\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131911\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131911\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.79 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.897236991273003, 6.056502943287204, 7.09689, 0.67559)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7826.55 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3194\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2696\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3125\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2696\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.63s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131911\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131913\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131913\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.78 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.903374617466499, 6.056502943287204, 7.08225, 0.67389)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7830.08 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2298\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2265\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2349\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2142\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.71s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131913\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131915\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131915\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.78 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.903374617466499, 6.0607107198447965, 7.10995, 0.66955)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7827.05 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2016\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2319\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2612\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2005\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.85s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131915\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131917\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131917\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.78 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.903374617466499, 6.056502943287204, 7.06764, 0.67533)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7827.78 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2163\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2232\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2352\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2115\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.93s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131917\\\")\n"," 81%|████████  | 81/100 [12:36<02:54,  9.20s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131919\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131919\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.77 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.58617432950102, 6.83186726194968, 7.63548, 0.49467)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7854.1 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1731\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1999\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2217\t = Validation score   (-SMAPE)\n","\t1.4s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1705\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.71s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131919\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131922\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131922\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.76 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.58617432950102, 6.83186726194968, 7.63793, 0.49518)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7820.85 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1617\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0010377\tvalid_set's SMAPE: -1.82864\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2428\t = Validation score   (-SMAPE)\n","\t0.84s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1616\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.28s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131922\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131924\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131924\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.75 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.58617432950102, 6.83186726194968, 7.63494, 0.48954)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7798.06 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1666\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1854\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2232\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1631\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.22s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131924\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131926\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131926\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.75 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.579431569251248, 6.83186726194968, 7.64544, 0.49013)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7824.6 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.000977065\tvalid_set's SMAPE: -1.66451\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.1563\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2056\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2467\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.156\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.25s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131926\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131929\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131929\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.74 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.58617432950102, 6.83186726194968, 7.6232, 0.49212)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7822.45 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1513\t = Validation score   (-SMAPE)\n","\t0.61s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1893\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2145\t = Validation score   (-SMAPE)\n","\t0.89s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1502\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.35s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131929\\\")\n"," 82%|████████▏ | 82/100 [12:48<03:03, 10.18s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131931\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131931\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.73 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.240596119367938, 6.44635453854367, 7.1388, 0.55387)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7832.15 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1424\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1751\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6281\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1408\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131931\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131933\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131933\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.73 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.210396255104774, 6.44635453854367, 7.1423, 0.55336)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7845.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1968\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00502667\tvalid_set's SMAPE: -1.56938\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.23\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2347\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.195\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.33s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131933\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131936\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131936\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.72 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.240596119367938, 6.456769655572163, 7.13732, 0.55014)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7847.01 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1674\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2101\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3917\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1638\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131936\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131937\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131937\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.72 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.240596119367938, 6.44635453854367, 7.14392, 0.55089)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7846.28 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.211\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.178\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1929\t = Validation score   (-SMAPE)\n","\t0.88s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1739\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.91s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131937\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131939\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131939\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.72 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.240596119367938, 6.44635453854367, 7.12395, 0.55008)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7858.0 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2812\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1986\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.305\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1976\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131939\\\")\n"," 83%|████████▎ | 83/100 [12:58<02:50, 10.05s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131941\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131941\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.71 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.222392508337796, 6.633792005337953, 7.36873, 0.45782)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7852.2 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1439\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1507\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1815\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1388\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.85s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131941\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131943\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131943\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.71 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.229201737777698, 6.633792005337953, 7.3738, 0.45866)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7853.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1252\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1536\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1852\t = Validation score   (-SMAPE)\n","\t0.99s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.124\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.38s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131943\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131946\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131946\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.70 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.229201737777698, 6.633792005337953, 7.36879, 0.45354)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7820.12 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1402\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.182\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1611\t = Validation score   (-SMAPE)\n","\t1.02s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1379\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.99s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131946\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131948\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131948\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.70 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.229201737777698, 6.634423086084641, 7.37919, 0.45231)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7806.7 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1372\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1586\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7692\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1364\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131948\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131949\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131949\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.69 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.229201737777698, 6.633792005337953, 7.35743, 0.45603)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7783.63 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1426\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1486\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1855\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1366\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.8s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131949\\\")\n"," 84%|████████▍ | 84/100 [13:08<02:41, 10.09s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131951\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131951\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.69 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.062155413060601, 7.295490809540891, 7.71282, 0.21354)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7792.5 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2328\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2179\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2108\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2106\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131951\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131953\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131953\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.69 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.062155413060601, 7.295490809540891, 7.71565, 0.21306)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7809.75 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.000787106\tvalid_set's SMAPE: -1.7121\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.1279\t = Validation score   (-SMAPE)\n","\t0.79s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1511\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2303\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1279\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.06s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131953\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131955\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131955\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.68 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.055932035144076, 7.305537616710878, 7.71417, 0.21101)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7813.73 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1274\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1818\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1744\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1274\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.67s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131955\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131957\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131957\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.68 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.062155413060601, 7.295490809540891, 7.7178, 0.21081)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7836.51 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1294\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1348\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1906\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1258\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.65s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131957\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_131959\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_131959\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.68 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.062155413060601, 7.295490809540891, 7.70793, 0.21442)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7844.53 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2312\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1422\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2132\t = Validation score   (-SMAPE)\n","\t0.62s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1422\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.6s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_131959\\\")\n"," 85%|████████▌ | 85/100 [13:17<02:26,  9.75s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132000\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132000\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.67 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.526803718114988, 6.1684803867784055, 6.95468, 0.46195)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7845.33 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3075\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2687\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3563\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2681\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.93s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132000\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132002\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132002\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.67 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.526803718114988, 6.166971268906583, 6.96422, 0.46063)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7851.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3498\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3493\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5083\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3335\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.06s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132002\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132004\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132004\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.66 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.526803718114988, 6.166971268906583, 6.95234, 0.46011)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7857.57 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.277\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.283\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.315\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2609\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.99s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132004\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132006\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132006\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.66 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.526803718114988, 6.166971268906583, 6.96735, 0.45841)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7843.73 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3119\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00602997\tvalid_set's SMAPE: -2.39401\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3201\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2942\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2857\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.1s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132006\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132009\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132009\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.65 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.52398901727311, 6.166971268906583, 6.9443, 0.46437)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7833.64 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2875\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00447934\tvalid_set's SMAPE: -2.40326\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2909\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3164\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2743\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.22s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132009\\\")\n"," 86%|████████▌ | 86/100 [13:28<02:21, 10.08s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132011\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132011\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.65 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.732711119113777, 5.8738065036397495, 7.03935, 0.56179)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7822.93 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.5097\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5012\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.526\t = Validation score   (-SMAPE)\n","\t0.99s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4875\t = Validation score   (-SMAPE)\n","\t0.12s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.19s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132011\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132013\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132013\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.64 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.762476943191248, 5.851800099980007, 7.0441, 0.56059)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7777.94 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4656\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.506\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5106\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4574\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132013\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132015\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132015\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.64 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.762476943191248, 5.851800099980007, 7.0387, 0.55804)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7788.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4879\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5115\t = Validation score   (-SMAPE)\n","\t0.48s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4772\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4557\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.98s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132015\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132017\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132017\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.63 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.762476943191248, 5.851800099980007, 7.04845, 0.55764)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7784.74 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4718\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5121\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5061\t = Validation score   (-SMAPE)\n","\t0.86s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4609\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.83s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132017\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132019\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132019\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.63 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.762476943191248, 5.851800099980007, 7.02383, 0.56572)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7771.14 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4122\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00790782\tvalid_set's SMAPE: -2.37351\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.4273\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4369\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4011\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.07s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132019\\\")\n"," 87%|████████▋ | 87/100 [13:38<02:12, 10.19s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132022\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132022\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.62 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.638130804632201, 6.019565977218857, 6.92755, 0.60408)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7793.55 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.283\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3016\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2642\t = Validation score   (-SMAPE)\n","\t0.87s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2552\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.97s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132022\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132024\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132024\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.62 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.637350100513128, 6.03734572236417, 6.93704, 0.60476)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7829.28 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2463\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00349768\tvalid_set's SMAPE: -2.26476\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3027\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2598\t = Validation score   (-SMAPE)\n","\t1.05s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2366\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.32s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132024\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132026\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132026\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.61 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.638130804632201, 6.019565977218857, 6.92399, 0.60322)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7777.14 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2814\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2679\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2724\t = Validation score   (-SMAPE)\n","\t0.9s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2526\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.05s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132026\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132028\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132028\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.60 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.638130804632201, 6.019565977218857, 6.94388, 0.6009)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7791.43 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.1s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.07s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2473\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2696\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2533\t = Validation score   (-SMAPE)\n","\t0.84s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2354\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.2s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132028\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132031\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132031\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.60 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.638130804632201, 6.019565977218857, 6.91291, 0.60681)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7801.14 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2517\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00468269\tvalid_set's SMAPE: -2.42143\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2652\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2652\t = Validation score   (-SMAPE)\n","\t0.78s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2405\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.13s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132031\\\")\n"," 88%|████████▊ | 88/100 [13:50<02:06, 10.53s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132033\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132033\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.59 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.918555870489848, 6.101842454577591, 7.09474, 0.63432)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7801.29 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2964\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3403\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3492\t = Validation score   (-SMAPE)\n","\t0.93s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2908\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.07s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132033\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132035\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132035\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.59 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.918555870489848, 6.101842454577591, 7.10374, 0.63699)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7849.23 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3302\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3679\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3561\t = Validation score   (-SMAPE)\n","\t0.83s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3231\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.83s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132035\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132037\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132037\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.58 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.918555870489848, 6.101842454577591, 7.08997, 0.63562)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7848.94 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3337\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3671\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3973\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3313\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.72s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132037\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132039\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132039\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.58 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.918555870489848, 6.101842454577591, 7.11366, 0.63119)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7834.74 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2899\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3165\t = Validation score   (-SMAPE)\n","\t0.4s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3046\t = Validation score   (-SMAPE)\n","\t0.91s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2757\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.01s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132039\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132041\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132041\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.57 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.914266898518613, 6.109869611486603, 7.08067, 0.63734)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7776.29 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3061\t = Validation score   (-SMAPE)\n","\t0.57s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.328\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3069\t = Validation score   (-SMAPE)\n","\t0.92s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2884\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.17s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132041\\\")\n"," 89%|████████▉ | 89/100 [14:00<01:55, 10.48s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132043\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132043\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.57 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.946461854024049, 5.848171377281505, 7.01751, 0.72057)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7761.63 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3542\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3755\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4509\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3416\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.15s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132043\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132045\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132045\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.56 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.933639156285308, 5.841919909827965, 7.02385, 0.7226)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7781.88 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3374\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4204\t = Validation score   (-SMAPE)\n","\t0.42s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.6339\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3357\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132045\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132047\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132047\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.56 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.946461854024049, 5.841919909827965, 7.00993, 0.7213)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7787.5 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.381\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3935\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8424\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3685\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.85s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132047\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132049\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132049\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.56 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.946461854024049, 5.841919909827965, 7.03397, 0.71968)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7780.88 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3825\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3785\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.456\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3622\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132049\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132051\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132051\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.55 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.946461854024049, 5.841919909827965, 6.99813, 0.72423)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7785.0 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3515\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3871\t = Validation score   (-SMAPE)\n","\t0.49s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4702\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3439\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132051\\\")\n"," 90%|█████████ | 90/100 [14:10<01:43, 10.33s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132053\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132053\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.55 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.901110750319267, 5.8882129172975075, 6.96275, 0.71247)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7790.11 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3663\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3843\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.407\t = Validation score   (-SMAPE)\n","\t0.84s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.359\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.11s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132053\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132055\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132055\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.54 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.901110750319267, 5.8882129172975075, 6.97376, 0.7124)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7799.85 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00499656\tvalid_set's SMAPE: -1.69007\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3617\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3918\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4256\t = Validation score   (-SMAPE)\n","\t0.86s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3592\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.41s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132055\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132058\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132058\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.53 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.901110750319267, 5.8882129172975075, 6.95843, 0.70912)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7777.18 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3658\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3924\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4745\t = Validation score   (-SMAPE)\n","\t0.73s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3625\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.87s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132058\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132100\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132100\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.53 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.901110750319267, 5.89087043187228, 6.98472, 0.7084)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7786.03 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4004\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3943\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3883\t = Validation score   (-SMAPE)\n","\t0.86s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3748\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.87s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132100\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132102\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132102\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.53 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.896031732628913, 5.8882129172975075, 6.94416, 0.70968)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7801.52 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00511983\tvalid_set's SMAPE: -1.69059\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3314\t = Validation score   (-SMAPE)\n","\t0.86s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00605358\tvalid_set's SMAPE: -1.68614\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3583\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.46\t = Validation score   (-SMAPE)\n","\t0.7s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3277\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.63s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132102\\\")\n"," 91%|█████████ | 91/100 [14:21<01:36, 10.69s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132105\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132105\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.52 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.663896037869864, 6.073228848910621, 7.02924, 0.53508)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7783.27 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2941\t = Validation score   (-SMAPE)\n","\t0.53s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3299\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.331\t = Validation score   (-SMAPE)\n","\t1.26s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2941\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.5s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132105\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132107\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132107\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.51 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.663896037869864, 6.073228848910621, 7.03803, 0.53362)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7714.55 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3024\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3119\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3097\t = Validation score   (-SMAPE)\n","\t0.84s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2898\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.04s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132107\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132110\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132110\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.50 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.663896037869864, 6.073228848910621, 7.02752, 0.53126)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7716.05 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2842\t = Validation score   (-SMAPE)\n","\t0.54s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3034\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2947\t = Validation score   (-SMAPE)\n","\t0.89s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2752\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.19s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132110\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132112\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132112\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.50 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.663896037869864, 6.138633770324603, 7.04532, 0.52828)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7723.9 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00315077\tvalid_set's SMAPE: -2.47095\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2553\t = Validation score   (-SMAPE)\n","\t0.98s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3138\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2771\t = Validation score   (-SMAPE)\n","\t1.07s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2503\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132112\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132115\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132115\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.49 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.657963090823695, 6.073228848910621, 7.01689, 0.53639)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7705.94 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2625\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00371003\tvalid_set's SMAPE: -2.54594\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3098\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2853\t = Validation score   (-SMAPE)\n","\t0.98s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2578\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.33s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132115\\\")\n"," 92%|█████████▏| 92/100 [14:34<01:30, 11.27s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132117\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132117\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.48 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.335203641625712, 6.298066504412274, 6.98493, 0.28146)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7708.01 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2338\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2391\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2443\t = Validation score   (-SMAPE)\n","\t0.86s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2281\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132117\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132119\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132119\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.48 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.335203641625712, 6.286742696181391, 6.98984, 0.2821)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7699.5 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.25\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3096\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2826\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2495\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.84s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132119\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132121\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132121\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.47 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.335203641625712, 6.286742696181391, 6.98771, 0.28097)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7724.66 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3024\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3172\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2821\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.279\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.65s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132121\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132123\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132123\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.47 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.335203641625712, 6.286742696181391, 6.99449, 0.27798)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7721.49 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3214\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.349\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2665\t = Validation score   (-SMAPE)\n","\t0.75s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2659\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.6s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132123\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132125\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132125\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.47 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.332617703500951, 6.286742696181391, 6.97677, 0.28403)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7735.47 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.332\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3318\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2738\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2738\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132125\\\")\n"," 93%|█████████▎| 93/100 [14:43<01:13, 10.54s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132126\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132126\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.46 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.255298416721173, 6.903546434208082, 7.78542, 0.28834)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7745.55 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2639\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2714\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.277\t = Validation score   (-SMAPE)\n","\t0.91s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2591\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.04s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132126\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132128\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132128\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.46 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.260958105101828, 6.903546434208082, 7.78897, 0.28809)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7759.79 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2607\t = Validation score   (-SMAPE)\n","\t0.44s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2755\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2845\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2576\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.83s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132128\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132130\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132130\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.45 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.260958105101828, 6.916239188630447, 7.78696, 0.28414)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7754.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2672\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2474\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2929\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2462\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.78s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132130\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132132\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132132\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.45 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.260958105101828, 6.903546434208082, 7.79367, 0.28413)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7767.38 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2585\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2648\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2871\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2535\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.99s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132132\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132134\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132134\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.45 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.260958105101828, 6.903546434208082, 7.77891, 0.28971)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7757.12 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3848\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3349\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3062\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3062\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.55s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132134\\\")\n"," 94%|█████████▍| 94/100 [14:53<01:01, 10.28s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132136\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132136\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.44 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.689261261679922, 6.1605319882235205, 6.97642, 0.28891)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7754.41 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8016\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8186\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8444\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8009\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.52s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132136\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132137\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132137\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.44 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.689261261679922, 6.1605319882235205, 6.98366, 0.28724)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7775.42 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.849\t = Validation score   (-SMAPE)\n","\t0.41s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8382\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.6954\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8367\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.61s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132137\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132139\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132139\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.44 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.689261261679922, 6.1605319882235205, 6.97856, 0.28591)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7778.68 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8842\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-1.7077\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8888\t = Validation score   (-SMAPE)\n","\t0.68s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8714\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.5s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132139\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132141\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132141\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.44 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.679824539546916, 6.355343479895216, 6.98305, 0.28774)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7787.98 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8516\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8833\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.8983\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.8504\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.59s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132141\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132142\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132142\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.44 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.689261261679922, 6.1605319882235205, 6.97092, 0.28782)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7783.92 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.8259\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.8468\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-1.6624\t = Validation score   (-SMAPE)\n","\t0.58s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.825\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.46s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132142\\\")\n"," 95%|█████████▌| 95/100 [15:01<00:47,  9.60s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132144\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132144\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.43 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.38497750430007, 7.192573253819268, 7.92408, 0.25434)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7810.4 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1784\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1854\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.197\t = Validation score   (-SMAPE)\n","\t0.77s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1738\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.81s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132144\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132146\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132146\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.43 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.38497750430007, 7.192573253819268, 7.92858, 0.25544)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7816.67 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2102\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2541\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2083\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2028\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132146\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132147\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132147\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.43 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.38497750430007, 7.192573253819268, 7.9266, 0.25407)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7810.45 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1985\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2097\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.1931\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1874\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.48s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132147\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132149\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132149\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.43 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.38497750430007, 7.196536746877219, 7.93457, 0.25192)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7807.75 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2064\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2227\t = Validation score   (-SMAPE)\n","\t0.36s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2356\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2055\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.57s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132149\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132151\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132151\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.42 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.374735218013281, 7.192573253819268, 7.91898, 0.25578)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7794.74 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2228\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1776\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2598\t = Validation score   (-SMAPE)\n","\t0.65s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1766\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.54s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132151\\\")\n"," 96%|█████████▌| 96/100 [15:09<00:36,  9.22s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132152\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132152\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.42 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.611862181778418, 6.234842005253001, 7.10028, 0.28296)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7793.48 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.301\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2822\t = Validation score   (-SMAPE)\n","\t0.51s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2888\t = Validation score   (-SMAPE)\n","\t1.02s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2763\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.19s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132152\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132155\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132155\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.41 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.611862181778418, 6.235547334435847, 7.10218, 0.28416)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7747.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2763\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2939\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3159\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2739\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.77s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132155\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132156\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132156\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.41 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.611862181778418, 6.234842005253001, 7.09904, 0.28173)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7746.57 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3141\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3109\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3329\t = Validation score   (-SMAPE)\n","\t0.72s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3042\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.58s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132156\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132158\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132158\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.41 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.611862181778418, 6.234842005253001, 7.11071, 0.27957)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7757.59 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.07s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3717\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2976\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3272\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2959\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.59s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132158\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132200\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132200\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.41 GB / 498.62 GB (25.0%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.598690013888154, 6.234842005253001, 7.09463, 0.28442)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7762.96 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2816\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2878\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3479\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2775\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.82s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132200\\\")\n"," 97%|█████████▋| 97/100 [15:18<00:27,  9.27s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132202\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132202\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.40 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.574127179705093, 5.770443741174751, 6.96184, 0.43394)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7737.91 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7572\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4932\t = Validation score   (-SMAPE)\n","\t0.55s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.4918\t = Validation score   (-SMAPE)\n","\t0.84s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4829\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.92s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132202\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132204\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132204\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.40 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.574127179705093, 5.770443741174751, 6.97385, 0.43206)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7770.47 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.7578\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5052\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5241\t = Validation score   (-SMAPE)\n","\t0.85s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4973\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.94s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132204\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132206\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132206\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.39 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.574127179705093, 5.770443741174751, 6.96945, 0.42796)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7760.05 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4567\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00766752\tvalid_set's SMAPE: -1.9884\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.5121\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5156\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4561\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.0s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132206\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132208\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132208\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.39 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.574127179705093, 5.827179352800126, 6.98227, 0.42614)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7753.76 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00730574\tvalid_set's SMAPE: -1.85219\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.4502\t = Validation score   (-SMAPE)\n","\t0.69s\t = Training   runtime\n","\t0.01s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.4897\t = Validation score   (-SMAPE)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5441\t = Validation score   (-SMAPE)\n","\t0.76s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4494\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.29s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132208\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132210\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132210\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.38 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.574127179705093, 5.770443741174751, 6.95444, 0.43683)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7742.21 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.635\t = Validation score   (-SMAPE)\n","\t0.29s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5669\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5981\t = Validation score   (-SMAPE)\n","\t0.6s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.5669\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.4s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132210\\\")\n"," 98%|█████████▊| 98/100 [15:28<00:18,  9.50s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132212\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132212\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.38 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.588262915995618, 6.2429234094022386, 7.01472, 0.34867)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7764.44 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1754\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2046\t = Validation score   (-SMAPE)\n","\t0.37s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2107\t = Validation score   (-SMAPE)\n","\t1.03s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1747\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.15s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132212\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132214\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132214\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.37 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.584610462765105, 6.2429234094022386, 7.01809, 0.34795)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7747.36 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1733\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.192\t = Validation score   (-SMAPE)\n","\t0.43s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2056\t = Validation score   (-SMAPE)\n","\t1.0s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1717\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.11s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132214\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132216\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132216\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.37 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.588262915995618, 6.2429234094022386, 7.01446, 0.34733)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7741.04 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1811\t = Validation score   (-SMAPE)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.2396\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2264\t = Validation score   (-SMAPE)\n","\t0.71s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1811\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.77s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132216\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132218\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132218\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.36 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.588262915995618, 6.258548425884557, 7.02529, 0.34349)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7732.9 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1704\t = Validation score   (-SMAPE)\n","\t0.52s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1795\t = Validation score   (-SMAPE)\n","\t0.46s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2453\t = Validation score   (-SMAPE)\n","\t0.66s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.167\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.9s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132218\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132220\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132220\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.36 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.588262915995618, 6.2429234094022386, 7.00573, 0.35063)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7771.83 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.1818\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.1894\t = Validation score   (-SMAPE)\n","\t0.39s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.2328\t = Validation score   (-SMAPE)\n","\t0.74s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.1789\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.7s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132220\\\")\n"," 99%|█████████▉| 99/100 [15:38<00:09,  9.68s/it]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132222\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132222\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.35 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.228127169256576, 5.672223005748435, 6.64009, 0.44126)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7776.2 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.2892\t = Validation score   (-SMAPE)\n","\t0.5s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.7209\t = Validation score   (-SMAPE)\n","\t0.3s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.9766\t = Validation score   (-SMAPE)\n","\t0.59s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2858\t = Validation score   (-SMAPE)\n","\t0.08s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.59s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132222\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132223\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132223\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.35 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1497\n","Train Data Columns: 23\n","Tuning Data Rows:    375\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.228127169256576, 5.672223005748435, 6.64552, 0.44141)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7776.84 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.05s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.351\t = Validation score   (-SMAPE)\n","\t0.35s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3539\t = Validation score   (-SMAPE)\n","\t0.38s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.642\t = Validation score   (-SMAPE)\n","\t0.64s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3426\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.56s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132223\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132225\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132225\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.35 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.223937340903819, 5.672223005748435, 6.64306, 0.43872)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7786.77 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3647\t = Validation score   (-SMAPE)\n","\t0.33s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3317\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.3597\t = Validation score   (-SMAPE)\n","\t1.0s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3275\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 2.06s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132225\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132227\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132227\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.34 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.228127169256576, 5.672223005748435, 6.65511, 0.4338)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7744.91 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.04s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.3839\t = Validation score   (-SMAPE)\n","\t0.31s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3301\t = Validation score   (-SMAPE)\n","\t0.45s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.5051\t = Validation score   (-SMAPE)\n","\t0.67s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.3258\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.64s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132227\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132229\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132229\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.34 GB / 498.62 GB (24.9%)\n","Train Data Rows:    1498\n","Train Data Columns: 23\n","Tuning Data Rows:    374\n","Tuning Data Columns: 23\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (7.228127169256576, 5.6804456092917635, 6.63186, 0.44345)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7726.56 MB\n","\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  5 | ['hour', 'day', 'month', 'week', 'holiday']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  4 | ['hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  1 | ['holiday']\n","\t0.0s = Fit runtime\n","\t23 features in original data used to generate 23 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n","Data preprocessing and feature engineering runtime = 0.06s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n","\t-0.4525\t = Validation score   (-SMAPE)\n","\t0.32s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.5753\t = Validation score   (-SMAPE)\n","\t0.34s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","\t-0.7065\t = Validation score   (-SMAPE)\n","\t0.63s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.4525\t = Validation score   (-SMAPE)\n","\t0.09s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 1.49s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132229\\\")\n","100%|██████████| 100/100 [15:47<00:00,  9.48s/it]\n"]},{"data":{"text/plain":["2.282570323855048"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["from tqdm import tqdm\n","\n","valid_df = train_x.copy()\n","valid_df['전력소비량(kWh)'] = 0\n","valid_df['pred'] = 0\n","pred_df = test_x.copy()\n","pred_df['전력소비량(kWh)'] = 0\n","\n","labels = []\n","\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n","\n","for i in tqdm(train_x['건물번호'].unique()):\n","    X = train_x[train_x['건물번호'] == i].drop(columns='건물번호')\n","    y = train_y[train_x['건물번호'] == i]\n","    valid_df.loc[train_x['건물번호'] == i, '전력소비량(kWh)'] = y\n","    y = np.log1p(y)\n","    x_test = test_x[test_x['건물번호'] == i].drop(columns='건물번호')\n","\n","    cols = X.columns[X.nunique() != 1]\n","    X = X[cols]\n","    x_test = x_test[cols]\n","\n","    tmp_preds = []\n","    for alpha in alphas:\n","\n","        model = XGBRegressor(random_state=seed, tree_method=\"gpu_hist\")\n","\n","        results = []\n","\n","        for fold_, (train_index, val_index) in enumerate(skf.split(X, X['day'])):\n","\n","            x_train, x_valid = X.iloc[train_index], X.iloc[val_index]\n","            y_train, y_valid = y.iloc[train_index], y.iloc[val_index]\n","\n","\n","            tr = pd.concat([x_train,y_train],axis=1)\n","            vl = pd.concat([x_valid,y_valid],axis=1)\n","\n","            train_data = TabularDataset(tr)\n","            test_data = TabularDataset(x_test)\n","\n","            predictor = TabularPredictor(label='전력소비량(kWh)',  eval_metric=mape_custom).fit(train_data,  tuning_data=vl, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, excluded_model_types = ['CAT', 'NN_TORCH' ,'XGB' ,'RF', 'FASTAI', 'KNN', 'XT'])\n","            pred = predictor.predict(test_data, model='LightGBMLarge')\n","            \n","            preds = np.expm1(pred)\n","            val_pred = predictor.predict(x_valid)\n","            valid_df.loc[x_valid.index, 'pred'] += np.expm1(val_pred)\n","            pred = predictor.predict(x_test)\n","            results.append(pred)\n","\n","        preds = np.mean(results, axis=0)\n","        preds = np.expm1(preds)\n","        tmp_preds.append(preds)\n","\n","    preds = np.mean(tmp_preds, axis=0)\n","    pred_df.loc[test_x['건물번호'] == i, '전력소비량(kWh)'] = preds\n","\n","    labels.append([valid_df.loc[train_x['건물번호'] == i, '전력소비량(kWh)'], preds])\n","\n","# 2.171737450532235\n","valid_df['pred'] = valid_df['pred'] / len(alphas)\n","val_score = SMAPE(valid_df['전력소비량(kWh)'], valid_df['pred'])\n","val_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/12 [00:00<?, ?it/s]No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132231\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132231\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.34 GB / 498.62 GB (24.9%)\n","Train Data Rows:    22464\n","Train Data Columns: 25\n","Tuning Data Rows:    5616\n","Tuning Data Columns: 25\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.948409213704661, 5.700711096131715, 7.47065, 0.51805)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7712.3 MB\n","\tTrain Data (Original)  Memory Usage: 5.5 MB (0.1% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  7 | ['건물번호', '태양광용량(kW)', 'hour', 'day', 'month', ...]\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  5 | ['건물번호', 'hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  2 | ['태양광용량(kW)', 'holiday']\n","\t0.1s = Fit runtime\n","\t25 features in original data used to generate 25 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 5.22 MB (0.1% of available memory)\n","Data preprocessing and feature engineering runtime = 0.09s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00670837\tvalid_set's SMAPE: -1.76724\n","[2000]\tvalid_set's l2: 0.00578718\tvalid_set's SMAPE: -1.76004\n","[3000]\tvalid_set's l2: 0.0053492\tvalid_set's SMAPE: -1.75439\n","[4000]\tvalid_set's l2: 0.00509966\tvalid_set's SMAPE: -1.7528\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3157\t = Validation score   (-SMAPE)\n","\t7.0s\t = Training   runtime\n","\t0.23s\t = Validation runtime\n","Fitting model: LightGBM ...\n","\t-0.3474\t = Validation score   (-SMAPE)\n","\t1.54s\t = Training   runtime\n","\t0.04s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00499095\tvalid_set's SMAPE: -1.75786\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2958\t = Validation score   (-SMAPE)\n","\t5.01s\t = Training   runtime\n","\t0.1s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2946\t = Validation score   (-SMAPE)\n","\t0.11s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 14.62s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132231\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132246\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132246\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.30 GB / 498.62 GB (24.9%)\n","Train Data Rows:    22464\n","Train Data Columns: 25\n","Tuning Data Rows:    5616\n","Tuning Data Columns: 25\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.948409213704661, 5.666426688112432, 7.47135, 0.51835)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7659.08 MB\n","\tTrain Data (Original)  Memory Usage: 5.5 MB (0.1% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  7 | ['건물번호', '태양광용량(kW)', 'hour', 'day', 'month', ...]\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  5 | ['건물번호', 'hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  2 | ['태양광용량(kW)', 'holiday']\n","\t0.1s = Fit runtime\n","\t25 features in original data used to generate 25 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 5.22 MB (0.1% of available memory)\n","Data preprocessing and feature engineering runtime = 0.1s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0061474\tvalid_set's SMAPE: -1.73021\n","[2000]\tvalid_set's l2: 0.00530542\tvalid_set's SMAPE: -1.72391\n","[3000]\tvalid_set's l2: 0.00491901\tvalid_set's SMAPE: -1.72086\n","[4000]\tvalid_set's l2: 0.00469188\tvalid_set's SMAPE: -1.72049\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.3156\t = Validation score   (-SMAPE)\n","\t5.86s\t = Training   runtime\n","\t0.19s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.0053398\tvalid_set's SMAPE: -1.7238\n","[2000]\tvalid_set's l2: 0.00480595\tvalid_set's SMAPE: -1.72073\n","[3000]\tvalid_set's l2: 0.00461059\tvalid_set's SMAPE: -1.71949\n","[4000]\tvalid_set's l2: 0.00450378\tvalid_set's SMAPE: -1.71872\n","[5000]\tvalid_set's l2: 0.00444232\tvalid_set's SMAPE: -1.71748\n","[6000]\tvalid_set's l2: 0.00439584\tvalid_set's SMAPE: -1.71633\n","[7000]\tvalid_set's l2: 0.00437453\tvalid_set's SMAPE: -1.71601\n","[8000]\tvalid_set's l2: 0.00435756\tvalid_set's SMAPE: -1.71577\n","[9000]\tvalid_set's l2: 0.00434402\tvalid_set's SMAPE: -1.71588\n","[10000]\tvalid_set's l2: 0.00433682\tvalid_set's SMAPE: -1.71594\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2937\t = Validation score   (-SMAPE)\n","\t12.5s\t = Training   runtime\n","\t0.38s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00492307\tvalid_set's SMAPE: -1.72092\n","[2000]\tvalid_set's l2: 0.0046571\tvalid_set's SMAPE: -1.71967\n","[3000]\tvalid_set's l2: 0.00460322\tvalid_set's SMAPE: -1.71887\n","[4000]\tvalid_set's l2: 0.0045816\tvalid_set's SMAPE: -1.71808\n","[5000]\tvalid_set's l2: 0.00457046\tvalid_set's SMAPE: -1.71824\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2884\t = Validation score   (-SMAPE)\n","\t12.84s\t = Training   runtime\n","\t0.28s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ...\n","\t-0.2839\t = Validation score   (-SMAPE)\n","\t0.1s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 33.47s ... Best model: \"WeightedEnsemble_L2\"\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230827_132246\\\")\n","No path specified. Models will be saved in: \"AutogluonModels\\ag-20230827_132322\\\"\n","Presets specified: ['medium_quality']\n","Beginning AutoGluon training ...\n","AutoGluon will save models to \"AutogluonModels\\ag-20230827_132322\\\"\n","AutoGluon Version:  0.8.2\n","Python Version:     3.8.6\n","Operating System:   Windows\n","Platform Machine:   AMD64\n","Platform Version:   10.0.19041\n","Disk Space Avail:   124.21 GB / 498.62 GB (24.9%)\n","Train Data Rows:    22464\n","Train Data Columns: 25\n","Tuning Data Rows:    5616\n","Tuning Data Columns: 25\n","Label Column: 전력소비량(kWh)\n","Preprocessing data ...\n","AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n","\tLabel info (max, min, mean, stddev): (8.948409213704661, 5.666426688112432, 7.46826, 0.52089)\n","\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    7803.17 MB\n","\tTrain Data (Original)  Memory Usage: 5.5 MB (0.1% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('float', []) : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])   :  7 | ['건물번호', '태양광용량(kW)', 'hour', 'day', 'month', ...]\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('float', [])     : 18 | ['기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', 'month_day_mean', ...]\n","\t\t('int', [])       :  5 | ['건물번호', 'hour', 'day', 'month', 'week']\n","\t\t('int', ['bool']) :  2 | ['태양광용량(kW)', 'holiday']\n","\t0.1s = Fit runtime\n","\t25 features in original data used to generate 25 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 5.22 MB (0.1% of available memory)\n","Data preprocessing and feature engineering runtime = 0.09s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'SMAPE'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': {},\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': {},\n","\t'XGB': {},\n","\t'FASTAI': {},\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","Excluded models: ['XGB', 'RF', 'KNN', 'CAT', 'NN_TORCH', 'XT', 'FASTAI'] (Specified by `excluded_model_types`)\n","Fitting 3 L1 models ...\n","Fitting model: LightGBMXT ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00628949\tvalid_set's SMAPE: -1.76469\n","[2000]\tvalid_set's l2: 0.00542288\tvalid_set's SMAPE: -1.75738\n","[3000]\tvalid_set's l2: 0.00503688\tvalid_set's SMAPE: -1.75611\n","[4000]\tvalid_set's l2: 0.00477849\tvalid_set's SMAPE: -1.75359\n","[5000]\tvalid_set's l2: 0.00460862\tvalid_set's SMAPE: -1.75225\n","[6000]\tvalid_set's l2: 0.00449098\tvalid_set's SMAPE: -1.75111\n","[7000]\tvalid_set's l2: 0.00440404\tvalid_set's SMAPE: -1.75121\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.2953\t = Validation score   (-SMAPE)\n","\t10.4s\t = Training   runtime\n","\t0.34s\t = Validation runtime\n","Fitting model: LightGBM ...\n"]},{"name":"stdout","output_type":"stream","text":["[1000]\tvalid_set's l2: 0.00533206\tvalid_set's SMAPE: -1.76393\n","[2000]\tvalid_set's l2: 0.00477932\tvalid_set's SMAPE: -1.76088\n"]},{"name":"stderr","output_type":"stream","text":["\t-0.308\t = Validation score   (-SMAPE)\n","\t3.18s\t = Training   runtime\n","\t0.09s\t = Validation runtime\n","Fitting model: LightGBMLarge ...\n","  0%|          | 0/12 [01:08<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\USER\\Desktop\\전력\\Electric_Final_Real_Ag.ipynb Cell 24\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%A0%84%EB%A0%A5/Electric_Final_Real_Ag.ipynb#X56sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m train_data \u001b[39m=\u001b[39m TabularDataset(tr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%A0%84%EB%A0%A5/Electric_Final_Real_Ag.ipynb#X56sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m test_data \u001b[39m=\u001b[39m TabularDataset(x_test)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%A0%84%EB%A0%A5/Electric_Final_Real_Ag.ipynb#X56sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m predictor \u001b[39m=\u001b[39m TabularPredictor(label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m전력소비량(kWh)\u001b[39m\u001b[39m'\u001b[39m,  eval_metric\u001b[39m=\u001b[39mmape_custom)\u001b[39m.\u001b[39mfit(train_data,  tuning_data\u001b[39m=\u001b[39mvl, presets\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmedium_quality\u001b[39m\u001b[39m'\u001b[39m,  ag_args_fit\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mnum_gpus\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m}, excluded_model_types \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mCAT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNN_TORCH\u001b[39m\u001b[39m'\u001b[39m ,\u001b[39m'\u001b[39m\u001b[39mXGB\u001b[39m\u001b[39m'\u001b[39m ,\u001b[39m'\u001b[39m\u001b[39mRF\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFASTAI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mKNN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mXT\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%A0%84%EB%A0%A5/Electric_Final_Real_Ag.ipynb#X56sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m pred \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mpredict(test_data, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLightGBMLarge\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%A0%84%EB%A0%A5/Electric_Final_Real_Ag.ipynb#X56sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m val_pred \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mpredict(x_valid)\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mgargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgkwargs)\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:986\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    987\u001b[0m     X\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m    988\u001b[0m     X_val\u001b[39m=\u001b[39;49mtuning_data,\n\u001b[0;32m    989\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[0;32m    990\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[0;32m    991\u001b[0m     num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds,\n\u001b[0;32m    992\u001b[0m     num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[0;32m    993\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    994\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    995\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    996\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    997\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    998\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    999\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m   1000\u001b[0m     verbosity\u001b[39m=\u001b[39;49mverbosity,\n\u001b[0;32m   1001\u001b[0m     use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout,\n\u001b[0;32m   1002\u001b[0m )\n\u001b[0;32m   1003\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[0;32m   1006\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1007\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[0;32m   1013\u001b[0m )\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[1;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, X_val\u001b[39m=\u001b[39;49mX_val, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:157\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m--> 157\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    158\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    159\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    160\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    161\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    162\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    163\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[0;32m    164\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit_trainer,\n\u001b[0;32m    165\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    166\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    167\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    168\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrainer_fit_kwargs,\n\u001b[0;32m    169\u001b[0m )\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[0;32m    171\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:114\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m log_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m20\u001b[39m, log_str)\n\u001b[1;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(\n\u001b[0;32m    115\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    116\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    117\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    118\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    119\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    120\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    121\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    122\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    123\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    124\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    125\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    126\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    127\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    128\u001b[0m )\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2371\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[0;32m   2369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_rows_val \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_val)\n\u001b[0;32m   2370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_cols_train \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m-> 2371\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_multi_levels(\n\u001b[0;32m   2372\u001b[0m     X,\n\u001b[0;32m   2373\u001b[0m     y,\n\u001b[0;32m   2374\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m   2375\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m   2376\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m   2377\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m   2378\u001b[0m     level_start\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2379\u001b[0m     level_end\u001b[39m=\u001b[39;49mnum_stack_levels \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2380\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2381\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2382\u001b[0m )\n\u001b[0;32m   2383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:395\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    393\u001b[0m         core_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_core)\n\u001b[0;32m    394\u001b[0m         aux_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_aux)\n\u001b[1;32m--> 395\u001b[0m     base_model_names, aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level(\n\u001b[0;32m    396\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    397\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    398\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    399\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    400\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    401\u001b[0m         models\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    402\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    403\u001b[0m         base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[0;32m    404\u001b[0m         core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs_level,\n\u001b[0;32m    405\u001b[0m         aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs_level,\n\u001b[0;32m    406\u001b[0m         name_suffix\u001b[39m=\u001b[39;49mname_suffix,\n\u001b[0;32m    407\u001b[0m         infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    408\u001b[0m         infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m     model_names_fit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m base_model_names \u001b[39m+\u001b[39m aux_models\n\u001b[0;32m    411\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_best \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(model_names_fit) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:539\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    537\u001b[0m     core_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[0;32m    538\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[1;32m--> 539\u001b[0m core_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level_core(\n\u001b[0;32m    540\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    541\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    542\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    543\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    544\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    545\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m    546\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    547\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    548\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    549\u001b[0m     base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[0;32m    550\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcore_kwargs,\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m \u001b[39mif\u001b[39;00m X_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_new_level_aux(\n\u001b[0;32m    555\u001b[0m         X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, base_model_names\u001b[39m=\u001b[39mcore_models, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, infer_limit\u001b[39m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maux_kwargs\n\u001b[0;32m    556\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:673\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m fit_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m    672\u001b[0m \u001b[39m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi(\n\u001b[0;32m    674\u001b[0m     X\u001b[39m=\u001b[39;49mX_init,\n\u001b[0;32m    675\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    676\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    677\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    678\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    679\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m    680\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    681\u001b[0m     stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[0;32m    682\u001b[0m     compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[0;32m    683\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m    684\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    685\u001b[0m )\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2321\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[39mif\u001b[39;00m n_repeat_start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2320\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m-> 2321\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_initial(\n\u001b[0;32m   2322\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   2323\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   2324\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m   2325\u001b[0m         k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[0;32m   2326\u001b[0m         n_repeats\u001b[39m=\u001b[39;49mn_repeats_initial,\n\u001b[0;32m   2327\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[0;32m   2328\u001b[0m         feature_prune_kwargs\u001b[39m=\u001b[39;49mfeature_prune_kwargs,\n\u001b[0;32m   2329\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2330\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2331\u001b[0m     )\n\u001b[0;32m   2332\u001b[0m     n_repeat_start \u001b[39m=\u001b[39m n_repeats_initial\n\u001b[0;32m   2333\u001b[0m     \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2160\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m bagged:\n\u001b[0;32m   2159\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 2160\u001b[0m     models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_fold(\n\u001b[0;32m   2161\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m   2162\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[0;32m   2163\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2164\u001b[0m         time_split\u001b[39m=\u001b[39;49mtime_split,\n\u001b[0;32m   2165\u001b[0m         time_ratio\u001b[39m=\u001b[39;49mtime_ratio,\n\u001b[0;32m   2166\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_args,\n\u001b[0;32m   2167\u001b[0m     )\n\u001b[0;32m   2168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2169\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2278\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2276\u001b[0m         time_start_model \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2277\u001b[0m         time_left \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time_start_model \u001b[39m-\u001b[39m time_start)\n\u001b[1;32m-> 2278\u001b[0m model_name_trained_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single_full(\n\u001b[0;32m   2279\u001b[0m     X, y, model, time_limit\u001b[39m=\u001b[39;49mtime_left, hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m   2280\u001b[0m )\n\u001b[0;32m   2282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m   2283\u001b[0m     \u001b[39mdel\u001b[39;00m model\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[0;32m   2047\u001b[0m         bagged_model_fit_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[0;32m   2048\u001b[0m             k_fold\u001b[39m=\u001b[39mk_fold, k_fold_start\u001b[39m=\u001b[39mk_fold_start, k_fold_end\u001b[39m=\u001b[39mk_fold_end, n_repeats\u001b[39m=\u001b[39mn_repeats, n_repeat_start\u001b[39m=\u001b[39mn_repeat_start\n\u001b[0;32m   2049\u001b[0m         )\n\u001b[0;32m   2050\u001b[0m         model_fit_kwargs\u001b[39m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[1;32m-> 2051\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_and_save(\n\u001b[0;32m   2052\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   2053\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   2054\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   2055\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m   2056\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m   2057\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m   2058\u001b[0m         stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[0;32m   2059\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   2060\u001b[0m         compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[0;32m   2061\u001b[0m         total_resources\u001b[39m=\u001b[39;49mtotal_resources,\n\u001b[0;32m   2062\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs,\n\u001b[0;32m   2063\u001b[0m     )\n\u001b[0;32m   2064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m   2065\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_trained\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1733\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[1;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1731\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1732\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1733\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single(X, y, model, X_val, y_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[0;32m   1735\u001b[0m fit_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   1736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_evaluation:\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1684\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[1;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train_single\u001b[39m(\u001b[39mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, total_resources\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AbstractModel:\n\u001b[0;32m   1680\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m \u001b[39m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[0;32m   1682\u001b[0m \u001b[39m    Returns trained model object.\u001b[39;00m\n\u001b[0;32m   1683\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1684\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[0;32m   1685\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:829\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_fit_resources(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    828\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_memory_usage(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 829\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    831\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\models\\lgb\\lgb_model.py:194\u001b[0m, in \u001b[0;36mLGBModel._fit\u001b[1;34m(self, X, y, X_val, y_val, time_limit, num_gpus, num_cpus, sample_weight, sample_weight_val, verbosity, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_column in param dict is overridden.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m train_lgb_model(early_stopping_callback_kwargs\u001b[39m=\u001b[39;49mearly_stopping_callback_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_params)\n\u001b[0;32m    195\u001b[0m \u001b[39mexcept\u001b[39;00m LightGBMError:\n\u001b[0;32m    196\u001b[0m     \u001b[39mif\u001b[39;00m train_params[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m:\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\models\\lgb\\lgb_utils.py:124\u001b[0m, in \u001b[0;36mtrain_lgb_model\u001b[1;34m(early_stopping_callback_kwargs, **train_params)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m booster\u001b[39m.\u001b[39mfit(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_params)\n\u001b[0;32m    123\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m lgb\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_params)\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[0;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#train_x_type[\"건물번호\"] = train_x_type[\"건물번호\"].astype(\"category\")\n","#test_x_type[\"건물번호\"] = test_x_type[\"건물번호\"].astype(\"category\")\n","\n","valid_df_type = train_x_type.copy()\n","valid_df_type['전력소비량(kWh)'] = 0\n","valid_df_type['pred'] = 0\n","pred_df_type = test_x_type.copy()\n","pred_df_type['전력소비량(kWh)'] = 0\n","\n","labels_type = []\n","\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n","\n","for i in tqdm(train_x_type['건물유형'].unique()):\n","    X = train_x_type[train_x_type['건물유형'] == i].drop(columns='건물유형')\n","    y = train_y[train_x_type['건물유형'] == i]\n","    valid_df_type.loc[train_x_type['건물유형'] == i, '전력소비량(kWh)'] = y\n","    y = np.log1p(y)\n","    x_test = test_x_type[test_x_type['건물유형'] == i].drop(columns='건물유형')\n","\n","    tmp_preds = []\n","    for alpha in alphas:\n","\n","        results = []\n","\n","        for fold_, (train_index, val_index) in enumerate(skf.split(X, X['day'])):\n","\n","            x_train, x_valid = X.iloc[train_index], X.iloc[val_index]\n","            y_train, y_valid = y.iloc[train_index], y.iloc[val_index]\n","            tr = pd.concat([x_train,y_train],axis=1)\n","            vl = pd.concat([x_valid,y_valid],axis=1)\n","\n","            train_data = TabularDataset(tr)\n","            test_data = TabularDataset(x_test)\n","\n","            predictor = TabularPredictor(label='전력소비량(kWh)',  eval_metric=mape_custom).fit(train_data,  tuning_data=vl, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, excluded_model_types = ['CAT', 'NN_TORCH' ,'XGB' ,'RF', 'FASTAI', 'KNN', 'XT'])\n","            pred = predictor.predict(test_data, model='LightGBMLarge')\n","\n","\n","            val_pred = predictor.predict(x_valid)\n","            valid_df_type.loc[x_valid.index, 'pred'] += np.expm1(val_pred)\n","            pred = predictor.predict(x_test)\n","            results.append(pred)\n","\n","\n","        preds = np.mean(results, axis=0)\n","        preds = np.expm1(preds)\n","        tmp_preds.append(preds)\n","\n","    preds = np.mean(tmp_preds, axis=0)\n","    pred_df_type.loc[test_x_type['건물유형'] == i, '전력소비량(kWh)'] = preds\n","\n","    labels_type.append([valid_df_type.loc[train_x_type['건물유형'] == i, '전력소비량(kWh)'], preds])\n","\n","# 3.462504576825228\n","valid_df_type['pred'] = valid_df_type['pred'] / len(alphas)\n","type_val_score = SMAPE(valid_df_type['전력소비량(kWh)'], valid_df_type['pred'])\n","type_val_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocVQtKEKfmZZ"},"outputs":[{"data":{"text/plain":["(2.2340621785608286, 2.748818609952282, 2.4914403942565553)"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["# (1.9531097965617121, 2.557635430408023, 2.2553726134848677)\n","# (1.9530552780909811, 2.5519303451985462, 2.2524928116447636)\n","# (1.7183125894172449, 2.287520125412716, 2.00291635741498)\n","val_score, type_val_score, np.mean([val_score, type_val_score])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","counts = 0\n","w = 0.97\n","# 1.9803304293430894\n","for num in train_df['건물번호'].unique():\n","    X = pd.concat([valid_df.loc[valid_df['건물번호'] == num, 'pred'], valid_df_type.loc[valid_df_type['건물번호'] == num, 'pred']], axis=1).values\n","    y = valid_df.loc[valid_df['건물번호'] == num, '전력소비량(kWh)']\n","    reg = LinearRegression().fit(X, y)\n","    p = reg.predict(X)\n","    score = SMAPE(valid_df.loc[valid_df['건물번호'] == num, '전력소비량(kWh)'], p)\n","    # print(f'{num}, score: {score}')\n","    pred_x = pd.concat([pred_df.loc[pred_df['건물번호'] == num, '전력소비량(kWh)'], pred_df_type.loc[pred_df['건물번호'] == num, '전력소비량(kWh)']], axis=1).values\n","    pred = reg.predict(pred_x)\n","\n","    threshold = pd.pivot_table(train_df.loc[train_df['건물번호'] == num], values = '전력소비량(kWh)', index = ['day', 'hour'], aggfunc = min).reset_index()\n","    df_pred = pred_df.loc[pred_df['건물번호'] == num].reset_index(drop=True)\n","    for j in range(len(pred)):\n","        min_power = threshold.loc[(threshold.day == df_pred.day[j])&(threshold.hour == df_pred.hour[j]), '전력소비량(kWh)'].values[0]\n","        if pred[j] < min_power:\n","            pred[j] = min_power\n","            counts += 1\n","\n","    threshold = pd.pivot_table(train_df.loc[train_df['건물번호'] == num], values = '전력소비량(kWh)', index = ['day', 'hour'], aggfunc = max).reset_index()\n","    df_pred = pred_df.loc[pred_df['건물번호'] == num].reset_index(drop=True)\n","    for j in range(len(pred)):\n","        max_power = threshold.loc[(threshold.day == df_pred.day[j])&(threshold.hour == df_pred.hour[j]), '전력소비량(kWh)'].values[0]\n","        if pred[j] > max_power:\n","            pred[j] = max_power\n","            counts += 1\n","\n","    pred_df.loc[pred_df['건물번호'] == num, '전력소비량(kWh)'] = pred\n","\n","    valid_df.loc[valid_df['건물번호'] == num, 'pred'] = p\n","\n","SMAPE(valid_df['전력소비량(kWh)'], valid_df['pred'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXpYOglnfmZa"},"outputs":[{"data":{"text/plain":["130"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["counts # 573 538"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(0.5516524931298885, 1.5506366284519835)"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["type_val_score / (val_score + type_val_score), SMAPE(valid_df['전력소비량(kWh)'], valid_df['pred'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-D8INsiafmZa"},"outputs":[{"data":{"text/plain":["(0.5530377787563981, 1.5445747552176554)"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["type_val_score / (val_score + type_val_score), SMAPE(valid_df['전력소비량(kWh)'], valid_df['pred'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Htc37CitfmZa"},"outputs":[],"source":["# plt.figure(figsize=(26, 300))\n","# for i in train_df['건물번호'].unique():\n","#     plt.subplot(train_df['건물번호'].nunique(), 1, i)\n","#     plt.title(i)\n","#     plt.plot(range(len(labels[i-1][0])), labels[i-1][0])\n","#     plt.plot(range(len(labels[i-1][0]), len(labels[i-1][0])+len(labels[i-1][1])), pred_df[pred_df['건물번호']==i]['전력소비량(kWh)'])\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8c1Pka6ffmZb"},"outputs":[],"source":["submission = pd.read_csv('sample_submission.csv')\n","submission['answer'] = pred_df['전력소비량(kWh)']\n","submission.to_csv('jh_21011928_AG.csv', index=False)\n","submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOhfSyC8rHXdEo+iEwKcf8j","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"nbformat":4,"nbformat_minor":0}
